{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"3D Tumor Lightsheet Analysis Pipeline This document provides instructions on operating a lightsheet imaging data analysis pipeline for extraction of quantitative readouts regarding vascular volume and drug penetration in tumors. Any technical questions regarding the use of this code can be submitted as a GitHub repository issue. Use case demonstrating the application of this tumor lightsheet image analysis pipeline can be found in the manuscript mentioned below: Learn more in our publication : Title: High-Resolution ex vivo tissue clearing, lightsheet imaging, and data analysis to support macromolecular drug and biomarker distribution in whole organs and tumors. Short Tile: 3D Histology applications for drug discovery. Authors: Niyanta Kumar, Petr Hroba\u0159, Martin Vagenknecht, Jindrich Soukup, Peter Bloomingdale, Tomoko Freshwater, Sophia Bardehle, Roman Peter, Nadia Patterson, Ruban Mangadu, Cinthia Pastuskovas, and Mark Cancilla Merck & Co., Inc.","title":"Home"},{"location":"#3d-tumor-lightsheet-analysis-pipeline","text":"This document provides instructions on operating a lightsheet imaging data analysis pipeline for extraction of quantitative readouts regarding vascular volume and drug penetration in tumors. Any technical questions regarding the use of this code can be submitted as a GitHub repository issue. Use case demonstrating the application of this tumor lightsheet image analysis pipeline can be found in the manuscript mentioned below: Learn more in our publication : Title: High-Resolution ex vivo tissue clearing, lightsheet imaging, and data analysis to support macromolecular drug and biomarker distribution in whole organs and tumors. Short Tile: 3D Histology applications for drug discovery. Authors: Niyanta Kumar, Petr Hroba\u0159, Martin Vagenknecht, Jindrich Soukup, Peter Bloomingdale, Tomoko Freshwater, Sophia Bardehle, Roman Peter, Nadia Patterson, Ruban Mangadu, Cinthia Pastuskovas, and Mark Cancilla Merck & Co., Inc.","title":"3D Tumor Lightsheet Analysis Pipeline"},{"location":"Code%20use%20pre-requisites%20and%20Installation%20instructions/","text":"Code use pre-requisites and Installation instructions Prior to running the tumor lightsheet image analysis pipeline, the following pre-requisites must be met: A) Installation of the python environment named 3d that contains all the necessary python packages and modules required to run this code. B) Organization of lightsheet data in the recommended folder architecture . C) Creation of a configuration.json file . Each of these pre-requisites are described in detail below. A) Installation of the required python environment: Guide for the installation can be found at this link B) Folder Structure Due to the large size of the lightsheet datasets, the code is written to allow output from each step of the analysis pipeline to directly save the results to the hard disk. These results can then be immediately loaded into memory. The main reason for this approach is the ability to perform the analysis on a local, moderately powerful computer. This overcomes the limitations imposed by limited RAM memory. As a consequence, it is necessary to use a fixed folder structure for the raw data and configuration file provided as an input. The recommended folder structure is illustrated with an example below: 3D_Tumor_Lightsheet_Analysis_Pipeline (can be replaced with your root directory name) * When analyzing one particular study, the following folders structure of three channels (vessels, tumors, virus) including the config.json file is expected. Expected Folder Structure The root directory is considered the 3D_Tumor_Lightsheet_Analysis_Pipeline folder (see tree diagram below) 3 D_Tumor_Lightsheet_Analysis_Pipeline \u2514\u2500 data \u2514\u2500 your_study_name \u2514\u2500 config . json \u2514\u2500 source \u2514\u2500 raw \u2514\u2500 tumor \u2502 \u2514\u2500 5 IT - 4 X_Ch2_z0300 . tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5 IT - 4 X_Ch2_z1300 . tiff \u251c\u2500 vessel \u2502 \u2514\u2500 5 IT - 4 X_Ch3_z0300 . tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5 IT - 4 X_Ch3_z1300 . tiff \u2502\u2500 virus \u2514\u2500 5 IT - 4 X_Ch1_z0300 . tiff \u2514\u2500 ... \u2514\u2500 5 IT - 4 X_Ch1_z1300 . tiff In this example the root directory is called \u20183D_Tumor_Lightsheet_Analysis_Pipeline\u2019. The lightsheet data is placed in a folder called \u2018data\u2019 and datasets can be organized by study name. For instance, in this example, \u20185IT\u2019 is a folder containing a lightsheet dataset for a single tumor. Within this folder there are sub-folders with tiff files for each of the three fluorescence channels detecting a marker of interest. tumor : this sub-folder has images with a cell nuclei stain such as syto16, which will be used to delineate the tumor boundary. vessel : this sub-folder has images obtained following immunohistochemical labeling using a marker such as CD31, which will be used to detect blood vessels. virus : this sub-folder has images obtained following immunohistochemical labeling using a marker to detect the biologic of interest. In our use case the biologic of interest is an oncolytic virus, and therefore the sub-folder is named as such. Each tiff file represents a single z plane within a specific marker/fluorescence channel. Therefore, it is recommended that file names incorporate information on study name, channel, and z plane information. E.g., 5IT-4X_Ch1_z0001, where: \u20185IT\u2019 represents study name \u20184X\u2019 represents imaging objective or can be other miscellaneous information. \u2018Ch1\u2019 represents the fluorescence channel z0001 represents the z plane Tiff files for each channel should have the same pixel resolution, format, and dimensions. NOTE : The above recommended folder structure is required only when using the fully automated operation mode of the analysis pipeline. However, we recommend utilizing this structure as a best practice even when using the code in a manual mode of operation where individual modules may be run separately allowing further customization. C) Creation of a configuration (.json file): A configuration file with a .json file extension needs to be included in the dataset folder, in order to provide input on data acquisition and analysis parameters. These parameters incude details such as voxel size, choice of segmentation method (e.g., thresholding vs machine-learning based) among others. A detailed list of relevant parameters that need to be specifed in the configuration file are listed below, followed by an example of the .json file itself. See table of arguments . D) Hardware Setting The analysis was performed on the high perfomance computing (HPC) environment. Due to the nature of the code (storing all middle steps localy) it may be used on local computer as well. 1) Our HPC recources HPC hardware we had at disposal is Cray CS Storm has following recources: 8 GPU Nodes. Each node in the following configuration: 8x V100 SXM2 32GB HBM2, NVLink 2 2x CLX 6240, 18c, 2.6 GHz (150W) 24x 32 GiB DDR4-2933; 768 GiB total 4x P4510, NVMe SSD, 2.5\u201d, 2 TB 2x S4510, SATA SSD, 2.5\u201d, 240 GB 4x Mellanox CX-4, x8, VPI Single-Port, QSFP28 On this setting we were able to analyze one study (containing roughly 1500 Z-planes) in 2 hour. (including resizing preprocessing) 2) Study Parameters One Lightsheet study has roufly 1500 z-plane images. Each image having resolution of 5732 x 6078 pixels (35 MB each). Therefore, one study has 3 x 1500 images requiring almost 1TB storage. For the purposes of analysis we resize the images to smaller ones and convert original tiff files to vector arrays. To operate the code on local computers we recommend the following MINIMAL Hardware Requirements: 3) Minimal Recommended setting for local usage Based on the input data we recommend following minimal recources: * CPU with 8 Cores * 16 GB RAM * 1TB (hard disk) Storage for the Data (for raw data storage) * GPU is only required when deep learning model (UNET) is being used.","title":"Code use pre requisites and Installation instructions"},{"location":"Code%20use%20pre-requisites%20and%20Installation%20instructions/#code-use-pre-requisites-and-installation-instructions","text":"Prior to running the tumor lightsheet image analysis pipeline, the following pre-requisites must be met: A) Installation of the python environment named 3d that contains all the necessary python packages and modules required to run this code. B) Organization of lightsheet data in the recommended folder architecture . C) Creation of a configuration.json file . Each of these pre-requisites are described in detail below.","title":"Code use pre-requisites and Installation instructions"},{"location":"Code%20use%20pre-requisites%20and%20Installation%20instructions/#a-installation-of-the-required-python-environment","text":"Guide for the installation can be found at this link","title":"A) Installation of the required python environment:"},{"location":"Code%20use%20pre-requisites%20and%20Installation%20instructions/#b-folder-structure","text":"Due to the large size of the lightsheet datasets, the code is written to allow output from each step of the analysis pipeline to directly save the results to the hard disk. These results can then be immediately loaded into memory. The main reason for this approach is the ability to perform the analysis on a local, moderately powerful computer. This overcomes the limitations imposed by limited RAM memory. As a consequence, it is necessary to use a fixed folder structure for the raw data and configuration file provided as an input. The recommended folder structure is illustrated with an example below: 3D_Tumor_Lightsheet_Analysis_Pipeline (can be replaced with your root directory name) * When analyzing one particular study, the following folders structure of three channels (vessels, tumors, virus) including the config.json file is expected. Expected Folder Structure The root directory is considered the 3D_Tumor_Lightsheet_Analysis_Pipeline folder (see tree diagram below) 3 D_Tumor_Lightsheet_Analysis_Pipeline \u2514\u2500 data \u2514\u2500 your_study_name \u2514\u2500 config . json \u2514\u2500 source \u2514\u2500 raw \u2514\u2500 tumor \u2502 \u2514\u2500 5 IT - 4 X_Ch2_z0300 . tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5 IT - 4 X_Ch2_z1300 . tiff \u251c\u2500 vessel \u2502 \u2514\u2500 5 IT - 4 X_Ch3_z0300 . tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5 IT - 4 X_Ch3_z1300 . tiff \u2502\u2500 virus \u2514\u2500 5 IT - 4 X_Ch1_z0300 . tiff \u2514\u2500 ... \u2514\u2500 5 IT - 4 X_Ch1_z1300 . tiff In this example the root directory is called \u20183D_Tumor_Lightsheet_Analysis_Pipeline\u2019. The lightsheet data is placed in a folder called \u2018data\u2019 and datasets can be organized by study name. For instance, in this example, \u20185IT\u2019 is a folder containing a lightsheet dataset for a single tumor. Within this folder there are sub-folders with tiff files for each of the three fluorescence channels detecting a marker of interest. tumor : this sub-folder has images with a cell nuclei stain such as syto16, which will be used to delineate the tumor boundary. vessel : this sub-folder has images obtained following immunohistochemical labeling using a marker such as CD31, which will be used to detect blood vessels. virus : this sub-folder has images obtained following immunohistochemical labeling using a marker to detect the biologic of interest. In our use case the biologic of interest is an oncolytic virus, and therefore the sub-folder is named as such. Each tiff file represents a single z plane within a specific marker/fluorescence channel. Therefore, it is recommended that file names incorporate information on study name, channel, and z plane information. E.g., 5IT-4X_Ch1_z0001, where: \u20185IT\u2019 represents study name \u20184X\u2019 represents imaging objective or can be other miscellaneous information. \u2018Ch1\u2019 represents the fluorescence channel z0001 represents the z plane Tiff files for each channel should have the same pixel resolution, format, and dimensions. NOTE : The above recommended folder structure is required only when using the fully automated operation mode of the analysis pipeline. However, we recommend utilizing this structure as a best practice even when using the code in a manual mode of operation where individual modules may be run separately allowing further customization.","title":"B) Folder Structure"},{"location":"Code%20use%20pre-requisites%20and%20Installation%20instructions/#c-creation-of-a-configuration-json-file","text":"A configuration file with a .json file extension needs to be included in the dataset folder, in order to provide input on data acquisition and analysis parameters. These parameters incude details such as voxel size, choice of segmentation method (e.g., thresholding vs machine-learning based) among others. A detailed list of relevant parameters that need to be specifed in the configuration file are listed below, followed by an example of the .json file itself. See table of arguments .","title":"C) Creation of a configuration (.json file):"},{"location":"Code%20use%20pre-requisites%20and%20Installation%20instructions/#d-hardware-setting","text":"The analysis was performed on the high perfomance computing (HPC) environment. Due to the nature of the code (storing all middle steps localy) it may be used on local computer as well.","title":"D) Hardware Setting"},{"location":"Code%20use%20pre-requisites%20and%20Installation%20instructions/#1-our-hpc-recources","text":"HPC hardware we had at disposal is Cray CS Storm has following recources: 8 GPU Nodes. Each node in the following configuration: 8x V100 SXM2 32GB HBM2, NVLink 2 2x CLX 6240, 18c, 2.6 GHz (150W) 24x 32 GiB DDR4-2933; 768 GiB total 4x P4510, NVMe SSD, 2.5\u201d, 2 TB 2x S4510, SATA SSD, 2.5\u201d, 240 GB 4x Mellanox CX-4, x8, VPI Single-Port, QSFP28 On this setting we were able to analyze one study (containing roughly 1500 Z-planes) in 2 hour. (including resizing preprocessing)","title":"1) Our HPC recources"},{"location":"Code%20use%20pre-requisites%20and%20Installation%20instructions/#2-study-parameters","text":"One Lightsheet study has roufly 1500 z-plane images. Each image having resolution of 5732 x 6078 pixels (35 MB each). Therefore, one study has 3 x 1500 images requiring almost 1TB storage. For the purposes of analysis we resize the images to smaller ones and convert original tiff files to vector arrays. To operate the code on local computers we recommend the following MINIMAL Hardware Requirements:","title":"2) Study Parameters"},{"location":"Code%20use%20pre-requisites%20and%20Installation%20instructions/#3-minimal-recommended-setting-for-local-usage","text":"Based on the input data we recommend following minimal recources: * CPU with 8 Cores * 16 GB RAM * 1TB (hard disk) Storage for the Data (for raw data storage) * GPU is only required when deep learning model (UNET) is being used.","title":"3) Minimal Recommended setting for local usage"},{"location":"Installation%20Process/","text":"Installation Process Repo is a python package. Installation process can be automated via bash .sh file in the environment_setup folder. 1) Code Download Clone the github repository (Entire project in one folder) by running: # Clone The repo localy to your computer git clone https://github.com/Merck/3D_Tumor_Lightsheet_Analysis_Pipeline.git # Navigate to the repo folder cd 3D_Tumor_Lightsheet_Analysis_Pipeline 2) Dependencies Installation : When installing the package: MAC/LINUX Users 2.1) Make sure you have conda installed on your computer. if not, you may use this link. 2.2) Create a python environment Run in the terminal: source environment_setup/set_3d_infrastructure.sh Windows Users 2.1) Make sure you have conda installed on your computer. if not, you may use this link. 2.2) Create a python environment run all lines of environment_setup/set_3d_infrastructure.sh manually in the terminal","title":"Installation Process"},{"location":"Installation%20Process/#installation-process","text":"Repo is a python package. Installation process can be automated via bash .sh file in the environment_setup folder.","title":"Installation Process"},{"location":"Installation%20Process/#1-code-download","text":"Clone the github repository (Entire project in one folder) by running: # Clone The repo localy to your computer git clone https://github.com/Merck/3D_Tumor_Lightsheet_Analysis_Pipeline.git # Navigate to the repo folder cd 3D_Tumor_Lightsheet_Analysis_Pipeline","title":"1) Code Download"},{"location":"Installation%20Process/#2-dependencies-installation","text":"When installing the package: MAC/LINUX Users 2.1) Make sure you have conda installed on your computer. if not, you may use this link. 2.2) Create a python environment Run in the terminal: source environment_setup/set_3d_infrastructure.sh Windows Users 2.1) Make sure you have conda installed on your computer. if not, you may use this link. 2.2) Create a python environment run all lines of environment_setup/set_3d_infrastructure.sh manually in the terminal","title":"2) Dependencies Installation:"},{"location":"Segmentation%20Quality/","text":"Supported segmentation methods are random forest , UNET and thresholding In the detailed analysis, Machine Learning based models seems to be providing the best results. 1) Comparision of segmentation methods 2) Unet Segmentation quality (3D view) 2) Corrective Characteristic of ML models Segmentation Machine Learning models allow for certain level of flexibility in the GroundTruth Data.","title":"Segmentation Quality"},{"location":"Segmentation%20Quality/#1-comparision-of-segmentation-methods","text":"","title":"1) Comparision of segmentation methods"},{"location":"Segmentation%20Quality/#2-unet-segmentation-quality-3d-view","text":"","title":"2) Unet Segmentation quality (3D view)"},{"location":"Segmentation%20Quality/#2-corrective-characteristic-of-ml-models","text":"Segmentation Machine Learning models allow for certain level of flexibility in the GroundTruth Data.","title":"2) Corrective Characteristic of ML models"},{"location":"code%20overview/","text":"The tumor lightsheet image analysis code is organized into 5 modules that include data pre-processing , segmentation , segmentation post-processing , distance transformation , and penetration profile generation . (See the pipeline overview HERE ) This code can be operated as either : An automated pipeline: which can analyze a dataset with minimal user input, or A manual pipeline: which allows each module in the pipeline to be run separately, allowing further user defined customization. See the sections on code modules HERE and code usage for more details HERE.","title":"Code overview"},{"location":"code%20usage/","text":"Code Usage The code can be operated in 2 separated ways: 1) Fully automated pipeline 2) Manual Usage of Modules 1)Fully automated pipeline which analyzes the entire study automaticaly and logs the results (final biologic penetration profiles) into mlflow. (All steps e.g. segmented masks are stored on the hard disk). This approach requires the usage of config.json file, which contains details regarding the pipeline settings (see config page) . Once the config.json is prepared, the code can be used via either of the two files bellow: main.py (Command line version) master_script.py (Script version which may be operated from jupyter lab or any other python IDE) main.py this file used for the CommandLine (terminal) interface. This file shall be used when the entire analysis should be performed automaticaly and one in intested only in inspecting the results in mlflow . Using a command line interface (terminal), the code can be run as: main module: module to run the whole pipleline main ( config_path ) Main Function for the entire pipeline for automated usage. It wraps the entire pipeline into one function which can be sourced from the terminal. Entire process is fully automated and uses config.json file. See Config file documentation Parameters config_path : (pathlib.PosixPath) Relative path to the config file defining the entire process (Notice the expected folder structure). 3D_Tumor_Lightsheet_Analysis_Pipeline \u2514\u2500 data \u2514\u2500 your_study_name \u2514\u2500 config.json \u2514\u2500 source \u2514\u2500raw \u2514\u2500tumor \u2502 \u2514\u2500 5IT-4X_Ch2_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch2_z1300.tiff \u251c\u2500vessel \u2502 \u2514\u2500 5IT-4X_Ch3_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch3_z1300.tiff \u2502\u2500virus \u2514\u2500 5IT-4X_Ch1_z0300.tiff \u2514\u2500 ... \u2514\u25005IT-4X_Ch1_z1300.tiff Returns Results are saved on the disk (segmented masks, distance transform) and saved to mlflow (if provided in config). Example Usage >>> conda activate 3 d >>> python main . py -- Config File : data / 5 IT_DUMMY_STUDY / config . json Source code in 3D_Tumor_Lightsheet_Analysis_Pipeline/main.py def main ( config_path : pathlib . PosixPath ) -> None : \"\"\" Main Function for the entire pipeline for automated usage. It wraps the entire pipeline into one function which can be sourced from the terminal. Entire process is fully automated and uses **config.json** file. See [Config file documentation](config.md) Parameters ---------- **config_path** : *(pathlib.PosixPath)* Relative path to the config file defining the entire process (Notice the expected folder structure). ```bash 3D_Tumor_Lightsheet_Analysis_Pipeline \u2514\u2500 data \u2514\u2500 your_study_name \u2514\u2500 config.json \u2514\u2500 source \u2514\u2500raw \u2514\u2500tumor \u2502 \u2514\u2500 5IT-4X_Ch2_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch2_z1300.tiff \u251c\u2500vessel \u2502 \u2514\u2500 5IT-4X_Ch3_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch3_z1300.tiff \u2502\u2500virus \u2514\u2500 5IT-4X_Ch1_z0300.tiff \u2514\u2500 ... \u2514\u25005IT-4X_Ch1_z1300.tiff ``` Returns ------ Results are saved on the disk (segmented masks, distance transform) and saved to mlflow (if provided in config). Example Usage -------------- ```python >>>conda activate 3d >>>python main.py --Config File: data/5IT_DUMMY_STUDY/config.json ``` \"\"\" # Config file config_path = root_directory_path . joinpath ( config_path ) experiment = load_config ( config_path ) # script name to log in MLFLOW SCRIPT_NAME = \"main.py\" ####################################### # # DATA PREPROCESSING # ##################################### experiment [ \"data\" ][ \"source\" ][ \"transformed\" ] = data_preprocessing_wrapper ( experiment [ \"data\" ][ \"source\" ][ \"raw\" ] ) ####################################### # # SEGMENTATION # ##################################### # segmentation of blood vessels out_path = segmentation_wrapper ( experiment [ \"data\" ][ \"source\" ][ \"transformed\" ][ \"vessel\" ], ** experiment [ \"segmentation_method_vessel\" ], ) experiment [ \"data\" ][ \"results\" ][ \"segmentation\" ][ \"vessel\" ] = out_path # segmentation of tumors out_path = segmentation_wrapper ( experiment [ \"data\" ][ \"source\" ][ \"transformed\" ][ \"tumor\" ], ** experiment [ \"segmentation_method_tumor\" ], ) experiment [ \"data\" ][ \"results\" ][ \"segmentation\" ][ \"tumor\" ] = out_path # # postprocessing tumor masks out_path = postprocess_masks ( experiment [ \"data\" ][ \"results\" ][ \"segmentation\" ][ \"tumor\" ], ** experiment [ \"segmentation_postprocessing_tumor\" ], ) experiment [ \"data\" ][ \"results\" ][ \"segmentation_postprocessing\" ][ \"tumor\" ] = out_path ####################################### # # DISTANCE TRANSFORM # ##################################### out_path = calculate_distance_tranform ( experiment [ \"data\" ][ \"results\" ][ \"segmentation\" ][ \"vessel\" ], ** experiment [ \"distance_tranform\" ][ \"method_parameters\" ], ) experiment [ \"data\" ][ \"results\" ][ \"distance_transform\" ][ \"vessel\" ] = out_path ####################################### # # PROFILE # ##################################### # calculating the final profile profiles = calculate_profile ( experiment [ \"data\" ][ \"source\" ][ \"transformed\" ][ \"virus\" ], experiment [ \"data\" ][ \"results\" ][ \"distance_transform\" ][ \"vessel\" ], experiment [ \"data\" ][ \"results\" ][ \"segmentation_postprocessing\" ][ \"tumor\" ], experiment [ \"pixels_to_microns\" ], force_overwrite = False , ) ####################################### # # ML-FLOW LOGGING # ##################################### print ( MLFLOW_EXPERIMENT_NAME ) if experiment [ \"mlflow_logging\" ]: mlflow_logging ( experiment , profiles , MLFLOW_TRACKING_URI , MLFLOW_EXPERIMENT_NAME , experiment [ \"mlflow_run_name\" ], SCRIPT_NAME , ) master_script.py This file is a script version of main.py file. This means that it allows for interactive (cell-by-cell) usage. 2)Manual Usage of Modules One can choose to run modules individually - e.g. only for blood vessels segmentations/distance transform. This allows for the further customization of the code. This approach does not require the config.json file. See the Documentation of individual modules for further details: Preprocessing, Segmentation, Postprocessing, Distance Transform, Profiles. Even when using Modules individually, the function's documentation can still be accesed within the jupyterlab. When using Contextual Helper we see the functions documentation (see below).","title":"Code usage"},{"location":"code%20usage/#code-usage","text":"The code can be operated in 2 separated ways: 1) Fully automated pipeline 2) Manual Usage of Modules 1)Fully automated pipeline which analyzes the entire study automaticaly and logs the results (final biologic penetration profiles) into mlflow. (All steps e.g. segmented masks are stored on the hard disk). This approach requires the usage of config.json file, which contains details regarding the pipeline settings (see config page) . Once the config.json is prepared, the code can be used via either of the two files bellow: main.py (Command line version) master_script.py (Script version which may be operated from jupyter lab or any other python IDE)","title":"Code Usage"},{"location":"code%20usage/#mainpy","text":"this file used for the CommandLine (terminal) interface. This file shall be used when the entire analysis should be performed automaticaly and one in intested only in inspecting the results in mlflow . Using a command line interface (terminal), the code can be run as: main module: module to run the whole pipleline","title":"main.py"},{"location":"code%20usage/#main.main","text":"Main Function for the entire pipeline for automated usage. It wraps the entire pipeline into one function which can be sourced from the terminal. Entire process is fully automated and uses config.json file. See Config file documentation","title":"main()"},{"location":"code%20usage/#main.main--parameters","text":"config_path : (pathlib.PosixPath) Relative path to the config file defining the entire process (Notice the expected folder structure). 3D_Tumor_Lightsheet_Analysis_Pipeline \u2514\u2500 data \u2514\u2500 your_study_name \u2514\u2500 config.json \u2514\u2500 source \u2514\u2500raw \u2514\u2500tumor \u2502 \u2514\u2500 5IT-4X_Ch2_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch2_z1300.tiff \u251c\u2500vessel \u2502 \u2514\u2500 5IT-4X_Ch3_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch3_z1300.tiff \u2502\u2500virus \u2514\u2500 5IT-4X_Ch1_z0300.tiff \u2514\u2500 ... \u2514\u25005IT-4X_Ch1_z1300.tiff","title":"Parameters"},{"location":"code%20usage/#main.main--returns","text":"Results are saved on the disk (segmented masks, distance transform) and saved to mlflow (if provided in config).","title":"Returns"},{"location":"code%20usage/#main.main--example-usage","text":">>> conda activate 3 d >>> python main . py -- Config File : data / 5 IT_DUMMY_STUDY / config . json Source code in 3D_Tumor_Lightsheet_Analysis_Pipeline/main.py def main ( config_path : pathlib . PosixPath ) -> None : \"\"\" Main Function for the entire pipeline for automated usage. It wraps the entire pipeline into one function which can be sourced from the terminal. Entire process is fully automated and uses **config.json** file. See [Config file documentation](config.md) Parameters ---------- **config_path** : *(pathlib.PosixPath)* Relative path to the config file defining the entire process (Notice the expected folder structure). ```bash 3D_Tumor_Lightsheet_Analysis_Pipeline \u2514\u2500 data \u2514\u2500 your_study_name \u2514\u2500 config.json \u2514\u2500 source \u2514\u2500raw \u2514\u2500tumor \u2502 \u2514\u2500 5IT-4X_Ch2_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch2_z1300.tiff \u251c\u2500vessel \u2502 \u2514\u2500 5IT-4X_Ch3_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch3_z1300.tiff \u2502\u2500virus \u2514\u2500 5IT-4X_Ch1_z0300.tiff \u2514\u2500 ... \u2514\u25005IT-4X_Ch1_z1300.tiff ``` Returns ------ Results are saved on the disk (segmented masks, distance transform) and saved to mlflow (if provided in config). Example Usage -------------- ```python >>>conda activate 3d >>>python main.py --Config File: data/5IT_DUMMY_STUDY/config.json ``` \"\"\" # Config file config_path = root_directory_path . joinpath ( config_path ) experiment = load_config ( config_path ) # script name to log in MLFLOW SCRIPT_NAME = \"main.py\" ####################################### # # DATA PREPROCESSING # ##################################### experiment [ \"data\" ][ \"source\" ][ \"transformed\" ] = data_preprocessing_wrapper ( experiment [ \"data\" ][ \"source\" ][ \"raw\" ] ) ####################################### # # SEGMENTATION # ##################################### # segmentation of blood vessels out_path = segmentation_wrapper ( experiment [ \"data\" ][ \"source\" ][ \"transformed\" ][ \"vessel\" ], ** experiment [ \"segmentation_method_vessel\" ], ) experiment [ \"data\" ][ \"results\" ][ \"segmentation\" ][ \"vessel\" ] = out_path # segmentation of tumors out_path = segmentation_wrapper ( experiment [ \"data\" ][ \"source\" ][ \"transformed\" ][ \"tumor\" ], ** experiment [ \"segmentation_method_tumor\" ], ) experiment [ \"data\" ][ \"results\" ][ \"segmentation\" ][ \"tumor\" ] = out_path # # postprocessing tumor masks out_path = postprocess_masks ( experiment [ \"data\" ][ \"results\" ][ \"segmentation\" ][ \"tumor\" ], ** experiment [ \"segmentation_postprocessing_tumor\" ], ) experiment [ \"data\" ][ \"results\" ][ \"segmentation_postprocessing\" ][ \"tumor\" ] = out_path ####################################### # # DISTANCE TRANSFORM # ##################################### out_path = calculate_distance_tranform ( experiment [ \"data\" ][ \"results\" ][ \"segmentation\" ][ \"vessel\" ], ** experiment [ \"distance_tranform\" ][ \"method_parameters\" ], ) experiment [ \"data\" ][ \"results\" ][ \"distance_transform\" ][ \"vessel\" ] = out_path ####################################### # # PROFILE # ##################################### # calculating the final profile profiles = calculate_profile ( experiment [ \"data\" ][ \"source\" ][ \"transformed\" ][ \"virus\" ], experiment [ \"data\" ][ \"results\" ][ \"distance_transform\" ][ \"vessel\" ], experiment [ \"data\" ][ \"results\" ][ \"segmentation_postprocessing\" ][ \"tumor\" ], experiment [ \"pixels_to_microns\" ], force_overwrite = False , ) ####################################### # # ML-FLOW LOGGING # ##################################### print ( MLFLOW_EXPERIMENT_NAME ) if experiment [ \"mlflow_logging\" ]: mlflow_logging ( experiment , profiles , MLFLOW_TRACKING_URI , MLFLOW_EXPERIMENT_NAME , experiment [ \"mlflow_run_name\" ], SCRIPT_NAME , )","title":"Example Usage"},{"location":"code%20usage/#master_scriptpy","text":"This file is a script version of main.py file. This means that it allows for interactive (cell-by-cell) usage. 2)Manual Usage of Modules One can choose to run modules individually - e.g. only for blood vessels segmentations/distance transform. This allows for the further customization of the code. This approach does not require the config.json file. See the Documentation of individual modules for further details: Preprocessing, Segmentation, Postprocessing, Distance Transform, Profiles. Even when using Modules individually, the function's documentation can still be accesed within the jupyterlab. When using Contextual Helper we see the functions documentation (see below).","title":"master_script.py"},{"location":"code_overview/","text":"Code Pipeline overview This document provides an overview of operating the code pipeline and the expected input of the data for the analysis. ```","title":"Code Pipeline overview"},{"location":"code_overview/#code-pipeline-overview","text":"","title":"Code Pipeline overview"},{"location":"code_overview/#this-document-provides-an-overview-of-operating-the-code-pipeline-and-the-expected-input-of-the-data-for-the-analysis","text":"```","title":"This document provides an overview of operating the code pipeline and the expected input of the data for the analysis."},{"location":"config/","text":"Config file contains an entire setting for the automated analysis via main.py file and master_script.py . It allows changing methods for blood vessels segmentation as well as changing pre-trained models to use. config.json file All Changeble parameters are listed bellow: Main Key method/value method_parameters segmentation_method_tumor 1 thresholding 2 method : [th_triangle,th_yen,th_otsu] segmentation_method_vessel 1 random_forest 2 model_file : relative path unet 2 model_file : relative path thresholding 2 method : [th_triangle,th_yen,th_otsu] segmentation_postprocessing_tumor split_tumor_into_core_and_periphery 3 periphery_as_ratio_of_max_distance :<0; 1> distance_tranform stack_size : int pixels_to_microns float mlflow_logging bool mlflow_run_name str Example of possible config.json file. { \"segmentation_method_tumor\" : { \"method\" : \"thresholding\" , # Select segmentation method for tumor channel (here \"thresholding\") \"method_parameters\" : { \"method\" : \"th_triangle\" # What thresholding method to use: yen, triangle, otsu... } } , \"segmentation_method_vessel\" : { \"method\" : \"unet\" , # What method to use for blood vessels segmentation \"method_parameters\" : { \"model_file\" : \"ppdm/data/unet_model.pt\" , # path to the pre-trained model } } , \"segmentation_postprocessing_tumor\" : { # Tumor postprocesing - splitting the brains to core and periphery \"method\" : \"split_tumor_into_core_and_periphery\" , # Split tumor to the core and periphery \"method_parameters\" : {} # No parameter necessary for this method - defaults to 0.2 (20 percent core, 80 periphery) } , \"distance_tranform\" : { # (Outer) Distance Transform for the blood vessels distance \"method_parameters\" : { # How many layers stacked together inside DT aggregation \"stack_size\" : 100 } } \"pixels_to_microns\" : 4 , # Multiplication constant for converting pixels to micrones \"mlflow_logging\" : true # Bool parameters if the results should be saved to mlflow. \"mlflow_run_name\" : \"DEMO\" # Name of the experiment which should be used for the logging } see pipeline overview \u21a9 \u21a9 see segmentation module documentation and examples \u21a9 \u21a9 \u21a9 \u21a9 Describes the fraction of tumor core and periphery (should add up to 1) - e.g. we assumed 0.2 (20 percent) periphery and 0.8 (80 percent) tumor core. \u21a9","title":"Config"},{"location":"config/#configjson-file","text":"All Changeble parameters are listed bellow: Main Key method/value method_parameters segmentation_method_tumor 1 thresholding 2 method : [th_triangle,th_yen,th_otsu] segmentation_method_vessel 1 random_forest 2 model_file : relative path unet 2 model_file : relative path thresholding 2 method : [th_triangle,th_yen,th_otsu] segmentation_postprocessing_tumor split_tumor_into_core_and_periphery 3 periphery_as_ratio_of_max_distance :<0; 1> distance_tranform stack_size : int pixels_to_microns float mlflow_logging bool mlflow_run_name str","title":"config.json file"},{"location":"config/#_1","text":"","title":""},{"location":"config/#example-of-possible-configjson-file","text":"{ \"segmentation_method_tumor\" : { \"method\" : \"thresholding\" , # Select segmentation method for tumor channel (here \"thresholding\") \"method_parameters\" : { \"method\" : \"th_triangle\" # What thresholding method to use: yen, triangle, otsu... } } , \"segmentation_method_vessel\" : { \"method\" : \"unet\" , # What method to use for blood vessels segmentation \"method_parameters\" : { \"model_file\" : \"ppdm/data/unet_model.pt\" , # path to the pre-trained model } } , \"segmentation_postprocessing_tumor\" : { # Tumor postprocesing - splitting the brains to core and periphery \"method\" : \"split_tumor_into_core_and_periphery\" , # Split tumor to the core and periphery \"method_parameters\" : {} # No parameter necessary for this method - defaults to 0.2 (20 percent core, 80 periphery) } , \"distance_tranform\" : { # (Outer) Distance Transform for the blood vessels distance \"method_parameters\" : { # How many layers stacked together inside DT aggregation \"stack_size\" : 100 } } \"pixels_to_microns\" : 4 , # Multiplication constant for converting pixels to micrones \"mlflow_logging\" : true # Bool parameters if the results should be saved to mlflow. \"mlflow_run_name\" : \"DEMO\" # Name of the experiment which should be used for the logging } see pipeline overview \u21a9 \u21a9 see segmentation module documentation and examples \u21a9 \u21a9 \u21a9 \u21a9 Describes the fraction of tumor core and periphery (should add up to 1) - e.g. we assumed 0.2 (20 percent) periphery and 0.8 (80 percent) tumor core. \u21a9","title":"Example of possible config.json file."},{"location":"methodology_overview/","text":"Methodology Overview A detailed description of the tumor lightsheet image analysis pipeline is provided in the methods section of the manuscript by Kumar et al. 2022 (largely inspired by the study 1 ). Diagram of Our Pipeline Analysis Pipeline Steps: Briefly, this tumor lightsheet image analysis method comprised of four main steps that accomplish: 1) Tumor blood vessel segmentation: The fluorescence channel detecting CD31 +ve tumor blood vessels in lightsheet images is segmented in order to obtain binary masks of the vessels. The binary masks then enable further quatification of vascular volume in the tumor and measurement of drug penetration away from tumor vasculature. Code implementation can be found in ( segmentation.py ) see segmentation module documentation and examples . 2) Tumor boundary segmentation: The fluorescence channel detecting Syto16 stained cell nuclei in lightsheet images is segmented to obtain a binary mask of the whole tumor to enable detection of the tumor boundary. The tumor region can also be divided into tumor core and periphery via further post-processing to enable comparison between physiologically relevant locations. Code implementation can be found in ( segmentation.py ) see segmentation module documentation and examples . 3) Vascular distance map creation: Following blood vessel and tumor tissue segmentation, we computed a distance map to characterize drug penetration from tumor blood vessels into neighboring tissue. In this step each pixel was assigned the distance to the nearest blood vessel. Documentation and examples of distance transform module ( distance_transform.py ) script. Documentation and examples of distance transfrom module . 4) Collect and aggregate data: In the final step of the tumor lightsheet data analysis pipeline, outputs from previous steps are aggregated to support interpretation. see profiles module documentation and examples . Refs Dobosz, M., Ntziachristos, V., Scheuer, W. & Strobel, S. Multispectral Fluorescence Ultramicroscopy: Three-Dimensional Visualization and Automatic Quantification of Tumor Morphology, Drug Penetration, and Antiangiogenic Treatment Response . Neoplasia 16, 1-U24, doi:10.1593/neo.131848 (2014).* \u21a9","title":"Methodology Overview"},{"location":"methodology_overview/#methodology-overview","text":"A detailed description of the tumor lightsheet image analysis pipeline is provided in the methods section of the manuscript by Kumar et al. 2022 (largely inspired by the study 1 ).","title":"Methodology Overview"},{"location":"methodology_overview/#diagram-of-our-pipeline","text":"","title":"Diagram of Our Pipeline"},{"location":"methodology_overview/#analysis-pipeline-steps","text":"Briefly, this tumor lightsheet image analysis method comprised of four main steps that accomplish:","title":"Analysis Pipeline Steps:"},{"location":"methodology_overview/#1-tumor-blood-vessel-segmentation","text":"The fluorescence channel detecting CD31 +ve tumor blood vessels in lightsheet images is segmented in order to obtain binary masks of the vessels. The binary masks then enable further quatification of vascular volume in the tumor and measurement of drug penetration away from tumor vasculature. Code implementation can be found in ( segmentation.py ) see segmentation module documentation and examples .","title":"1) Tumor blood vessel segmentation:"},{"location":"methodology_overview/#2-tumor-boundary-segmentation","text":"The fluorescence channel detecting Syto16 stained cell nuclei in lightsheet images is segmented to obtain a binary mask of the whole tumor to enable detection of the tumor boundary. The tumor region can also be divided into tumor core and periphery via further post-processing to enable comparison between physiologically relevant locations. Code implementation can be found in ( segmentation.py ) see segmentation module documentation and examples .","title":"2) Tumor boundary segmentation:"},{"location":"methodology_overview/#3-vascular-distance-map-creation","text":"Following blood vessel and tumor tissue segmentation, we computed a distance map to characterize drug penetration from tumor blood vessels into neighboring tissue. In this step each pixel was assigned the distance to the nearest blood vessel. Documentation and examples of distance transform module ( distance_transform.py ) script. Documentation and examples of distance transfrom module .","title":"3) Vascular distance map creation:"},{"location":"methodology_overview/#4-collect-and-aggregate-data","text":"In the final step of the tumor lightsheet data analysis pipeline, outputs from previous steps are aggregated to support interpretation. see profiles module documentation and examples .","title":"4) Collect and aggregate data:"},{"location":"methodology_overview/#_1","text":"","title":""},{"location":"methodology_overview/#refs","text":"Dobosz, M., Ntziachristos, V., Scheuer, W. & Strobel, S. Multispectral Fluorescence Ultramicroscopy: Three-Dimensional Visualization and Automatic Quantification of Tumor Morphology, Drug Penetration, and Antiangiogenic Treatment Response . Neoplasia 16, 1-U24, doi:10.1593/neo.131848 (2014).* \u21a9","title":"Refs"},{"location":"Modules/distance_transform/","text":"Distance Transform distance transform module: module for computing distance transform on segmented masks This module consists of one main functions: calculate_distance_tranform calculate_distance_tranform ( data_path , stack_size = 100 ) Wrapper function that calculates the distance transform on the segmented blood vessels masks. Due to the size of the data it calculated the distance transform using overlapping cubes. Parameters data_path : (pathlib.PosixPath) relative path for the segmented masks to be used for the distance transform stack_size : (int) For the HPC setting we used stack size of 100. For local PC we recommend smaller stack size. HERE Returns output_directory : *(pathlib.PosixPath) outputs relative path for the folders with calculated distance transform. Results are stored on the disk. Example Usage >>> from src.distance_tranform import calculate_distance_tranform >>> segmented_vessels_path = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/segmentation/vessel/segment___unet___model_file-model' ) >>> calculate_distance_tranform ( study_paths ) >>> output : Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/distance_transform/vessel/distance_tranform___segment___unet___model_file-model' ) Source code in src/distance_transform.py @log_step def calculate_distance_tranform ( data_path : pathlib . PosixPath , stack_size : int = 100 ) -> pathlib . PosixPath : \"\"\" Wrapper function that calculates the distance transform on the segmented blood vessels masks. Due to the size of the data it calculated the distance transform using overlapping cubes. Parameters ---------- **data_path**: *(pathlib.PosixPath)* relative path for the segmented masks to be used for the distance transform **stack_size**: *(int)* For the HPC setting we used stack size of 100. For local PC we recommend smaller stack size. [HERE](Code use pre-requisites and Installation instructions.md) Returns ------ **output_directory**: *(pathlib.PosixPath) outputs relative path for the folders with calculated distance transform. Results are stored on the disk. Example Usage -------------- ```python >>>from src.distance_tranform import calculate_distance_tranform >>>segmented_vessels_path = Path('ppdm/data/5IT_DUMMY_STUDY/results/segmentation/vessel/segment___unet___model_file-model') >>>calculate_distance_tranform(study_paths) >>>output: Path('ppdm/data/5IT_DUMMY_STUDY/results/distance_transform/vessel/distance_tranform___segment___unet___model_file-model') ``` \"\"\" module_results_path = \"results/distance_transform\" module_name = \"distance_tranform\" output_folder_name = join_to_string ([ module_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) directory_ok = check_content_of_two_directories ( data_path , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) # step1 - create overlapping bricks submodule_name = \"tmp_overlapping_bricks\" output_folder_name = join_to_string ([ submodule_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) if output_directory . exists (): remove_content ( output_directory ) overlapping_bricks = _merge_with_overlap ( data_path , output_directory , stack_size = stack_size ) # step2 - calculate distance transform on overlapping brikcs submodule_name = \"tmp_dt_overlapping_bricks\" output_folder_name = join_to_string ([ submodule_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) if output_directory . exists (): remove_content ( output_directory ) overlapping_bricks_dt = _compute_distance_transform ( overlapping_bricks , output_directory ) # step3 - aggregate information from overlapping bricks submodule_name = \"tmp_dt_aggregated_bricks\" output_folder_name = join_to_string ([ submodule_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) if output_directory . exists (): remove_content ( output_directory ) aggregated_bricks_dt = _aggregate ( overlapping_bricks_dt , output_directory ) # final step4 - slice bricks back into individual layers output_folder_name = join_to_string ([ module_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) if output_directory . exists (): remove_content ( output_directory ) names_of_individual_layers = data_path _split ( aggregated_bricks_dt , output_directory , names_of_individual_layers ) return output_directory","title":"Distance Transform"},{"location":"Modules/distance_transform/#distance-transform","text":"","title":"Distance Transform"},{"location":"Modules/distance_transform/#src.distance_transform--distance-transform-module","text":"","title":"distance transform module:"},{"location":"Modules/distance_transform/#src.distance_transform--module-for-computing-distance-transform-on-segmented-masks","text":"","title":"module for computing distance transform on segmented masks"},{"location":"Modules/distance_transform/#src.distance_transform--this-module-consists-of-one-main-functions","text":"calculate_distance_tranform","title":"This module consists of one main functions:"},{"location":"Modules/distance_transform/#src.distance_transform.calculate_distance_tranform","text":"Wrapper function that calculates the distance transform on the segmented blood vessels masks. Due to the size of the data it calculated the distance transform using overlapping cubes.","title":"calculate_distance_tranform()"},{"location":"Modules/distance_transform/#src.distance_transform.calculate_distance_tranform--parameters","text":"data_path : (pathlib.PosixPath) relative path for the segmented masks to be used for the distance transform stack_size : (int) For the HPC setting we used stack size of 100. For local PC we recommend smaller stack size. HERE","title":"Parameters"},{"location":"Modules/distance_transform/#src.distance_transform.calculate_distance_tranform--returns","text":"output_directory : *(pathlib.PosixPath) outputs relative path for the folders with calculated distance transform. Results are stored on the disk.","title":"Returns"},{"location":"Modules/distance_transform/#src.distance_transform.calculate_distance_tranform--example-usage","text":">>> from src.distance_tranform import calculate_distance_tranform >>> segmented_vessels_path = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/segmentation/vessel/segment___unet___model_file-model' ) >>> calculate_distance_tranform ( study_paths ) >>> output : Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/distance_transform/vessel/distance_tranform___segment___unet___model_file-model' ) Source code in src/distance_transform.py @log_step def calculate_distance_tranform ( data_path : pathlib . PosixPath , stack_size : int = 100 ) -> pathlib . PosixPath : \"\"\" Wrapper function that calculates the distance transform on the segmented blood vessels masks. Due to the size of the data it calculated the distance transform using overlapping cubes. Parameters ---------- **data_path**: *(pathlib.PosixPath)* relative path for the segmented masks to be used for the distance transform **stack_size**: *(int)* For the HPC setting we used stack size of 100. For local PC we recommend smaller stack size. [HERE](Code use pre-requisites and Installation instructions.md) Returns ------ **output_directory**: *(pathlib.PosixPath) outputs relative path for the folders with calculated distance transform. Results are stored on the disk. Example Usage -------------- ```python >>>from src.distance_tranform import calculate_distance_tranform >>>segmented_vessels_path = Path('ppdm/data/5IT_DUMMY_STUDY/results/segmentation/vessel/segment___unet___model_file-model') >>>calculate_distance_tranform(study_paths) >>>output: Path('ppdm/data/5IT_DUMMY_STUDY/results/distance_transform/vessel/distance_tranform___segment___unet___model_file-model') ``` \"\"\" module_results_path = \"results/distance_transform\" module_name = \"distance_tranform\" output_folder_name = join_to_string ([ module_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) directory_ok = check_content_of_two_directories ( data_path , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) # step1 - create overlapping bricks submodule_name = \"tmp_overlapping_bricks\" output_folder_name = join_to_string ([ submodule_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) if output_directory . exists (): remove_content ( output_directory ) overlapping_bricks = _merge_with_overlap ( data_path , output_directory , stack_size = stack_size ) # step2 - calculate distance transform on overlapping brikcs submodule_name = \"tmp_dt_overlapping_bricks\" output_folder_name = join_to_string ([ submodule_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) if output_directory . exists (): remove_content ( output_directory ) overlapping_bricks_dt = _compute_distance_transform ( overlapping_bricks , output_directory ) # step3 - aggregate information from overlapping bricks submodule_name = \"tmp_dt_aggregated_bricks\" output_folder_name = join_to_string ([ submodule_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) if output_directory . exists (): remove_content ( output_directory ) aggregated_bricks_dt = _aggregate ( overlapping_bricks_dt , output_directory ) # final step4 - slice bricks back into individual layers output_folder_name = join_to_string ([ module_name , data_path . stem ]) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) if output_directory . exists (): remove_content ( output_directory ) names_of_individual_layers = data_path _split ( aggregated_bricks_dt , output_directory , names_of_individual_layers ) return output_directory","title":"Example Usage"},{"location":"Modules/preprocessing/","text":"Preprocessing Preprocessing module: Converting Images to numpy form and resizing This module consists of two main functions: convert_images_to_numpy_format data_preprocessing_wrapper convert_images_to_numpy_format ( input_directory ) wrapper function that reduces images in the input folder and converts content of folder to numpy format in parallel (faster) Parameters input_directory : (pathlib.PosixPath object) relative path to the images which should be preprocessed Returns output_directory : (pathlib.PosixPath object) relative path for the folder with preprocessed results (tranferred and converted images) Example Usage >>> from src.preprocessing import convert_images_to_numpy_format >>> tumor_folder = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/raw/tumor' ) >>> convert_images_to_numpy_format ( tumor_folder ) >>> output : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/tumor' ) Source code in src/preprocessing.py def convert_images_to_numpy_format ( input_directory : pathlib . PosixPath , ) -> pathlib . PosixPath : \"\"\" wrapper function that reduces images in the input folder and converts content of folder to numpy format in parallel (faster) Parameters ---------- **input_directory**: *(pathlib.PosixPath object)* relative path to the images which should be preprocessed Returns ------ **output_directory**: *(pathlib.PosixPath object)* relative path for the folder with preprocessed results (tranferred and converted images) Example Usage -------------- ```python >>>from src.preprocessing import convert_images_to_numpy_format >>>tumor_folder = Path('ppdm/data/5IT_DUMMY_STUDY/source/raw/tumor') >>>convert_images_to_numpy_format(tumor_folder) >>>output: Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/tumor') ``` \"\"\" output_directory = create_preprocessing_relat_directory ( input_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) directory_ok = check_content_of_two_directories ( input_directory , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) parallel_create_numpy_formats_of_input_img = partial ( _create_numpy_formats_of_input_img , output_directory = output_directory , ) parallel ( parallel_create_numpy_formats_of_input_img , sorted ( list ( input_directory . glob ( \"*.tiff\" ))), n_workers = 12 , progress = True , threadpool = True , ) return output_directory data_preprocessing_wrapper ( data ) High level function that covers preprocessing for all data (blood vessels, tumors, virus). It used the convert_images_to_numpy_format function and applies to each channel (blood vessels, tumors, virus). If you want to preprocess just one channel (e.g. only tumors) use the convert_images_to_numpy_format function. Parameters data : (dict) containing keys (names of the channels) and values (relative paths to it). Returns preprocessed_data : *(dict) outputs three relative paths for the folders with preprocessed results (tranferred and converted images) Results are stored on the disk. Example Usage >>> from src.preprocessing import data_preprocessing_wrapper >>> study_paths = { 'vessel' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/raw/vessel' ), 'tumor' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/raw/tumor' ), 'virus' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/raw/virus' )} >>> data_preprocessing_wrapper ( study_paths ) >>> output : defaultdict ( < function src . utils . nested_dict () > , { 'vessel' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), 'tumor' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/tumor' ), 'virus' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/virus' )}) Source code in src/preprocessing.py @log_step def data_preprocessing_wrapper ( data : dict ) -> dict : \"\"\" High level function that covers preprocessing for all data (blood vessels, tumors, virus). It used the convert_images_to_numpy_format function and applies to each channel (blood vessels, tumors, virus). If you want to preprocess just one channel (e.g. only tumors) use the convert_images_to_numpy_format function. Parameters ---------- **data**: *(dict)* containing keys (names of the channels) and values (relative paths to it). Returns ------ **preprocessed_data**: *(dict) outputs three relative paths for the folders with preprocessed results (tranferred and converted images) Results are stored on the disk. Example Usage -------------- ```python >>>from src.preprocessing import data_preprocessing_wrapper >>>study_paths = {'vessel': Path('ppdm/data/5IT_DUMMY_STUDY/source/raw/vessel'), 'tumor': Path('ppdm/data/5IT_DUMMY_STUDY/source/raw/tumor'), 'virus': Path('ppdm/data/5IT_DUMMY_STUDY/source/raw/virus')} >>>data_preprocessing_wrapper(study_paths) >>>output: defaultdict(<function src.utils.nested_dict()>, {'vessel': Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), 'tumor': Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/tumor'), 'virus': Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/virus')}) ``` \"\"\" preprocessed_data = nested_dict () out_path = convert_images_to_numpy_format ( data [ \"vessel\" ]) preprocessed_data [ \"vessel\" ] = out_path # tumor out_path = convert_images_to_numpy_format ( data [ \"tumor\" ]) preprocessed_data [ \"tumor\" ] = out_path # virus out_path = convert_images_to_numpy_format ( data [ \"virus\" ]) preprocessed_data [ \"virus\" ] = out_path return preprocessed_data This step will transform and downsize input data ( tumor , vessel , virus ) .tiff files into python's numpy array files, which will be saved to the output path directory. The new folder structure will look as follows: ppdm \u2514\u2500 data \u2514\u2500 5IT_STUDY \u2514\u2500 config.json \u2514\u2500 source \u2514\u2500raw \u2502 \u2514\u2500tumor \u2502 \u2502 \u2514\u2500 5IT-4X_Ch2_z0300.tiff \u2502 \u2502 \u2514\u2500 ... \u2502 \u2502 \u2514\u2500 5IT-4X_Ch2_z1300.tiff \u2502 \u251c\u2500vessel \u2502 \u2502 \u2514\u2500 5IT-4X_Ch3_z0300.tiff \u2502 \u2502 \u2514\u2500 ... \u2502 \u2502 \u2514\u2500 5IT-4X_Ch3_z1300.tiff \u2502 \u2502\u2500virus \u2502 \u2514\u2500 5IT-4X_Ch1_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u25005IT-4X_Ch1_z1300.tiff ------------\u2502------------------------------------------------------- \u2514\u2500transformed \u2514\u2500 np_and_resized \u2514\u2500tumor \u2502 \u2514\u2500 5IT-4X_Ch2_z0300.np \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch2_z1300.np \u251c\u2500vessel \u2502 \u2514\u2500 5IT-4X_Ch3_z0300.np \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch3_z1300.np \u2502\u2500virus \u2514\u2500 5IT-4X_Ch1_z0300.np \u2514\u2500 ... \u2514\u25005IT-4X_Ch1_z1300.np","title":"Preprocessing"},{"location":"Modules/preprocessing/#preprocessing","text":"","title":"Preprocessing"},{"location":"Modules/preprocessing/#src.preprocessing--preprocessing-module","text":"","title":"Preprocessing module:"},{"location":"Modules/preprocessing/#src.preprocessing--converting-images-to-numpy-form-and-resizing","text":"","title":"Converting Images to numpy form and resizing"},{"location":"Modules/preprocessing/#src.preprocessing--this-module-consists-of-two-main-functions","text":"convert_images_to_numpy_format data_preprocessing_wrapper","title":"This module consists of two main functions:"},{"location":"Modules/preprocessing/#src.preprocessing.convert_images_to_numpy_format","text":"wrapper function that reduces images in the input folder and converts content of folder to numpy format in parallel (faster)","title":"convert_images_to_numpy_format()"},{"location":"Modules/preprocessing/#src.preprocessing.convert_images_to_numpy_format--parameters","text":"input_directory : (pathlib.PosixPath object) relative path to the images which should be preprocessed","title":"Parameters"},{"location":"Modules/preprocessing/#src.preprocessing.convert_images_to_numpy_format--returns","text":"output_directory : (pathlib.PosixPath object) relative path for the folder with preprocessed results (tranferred and converted images)","title":"Returns"},{"location":"Modules/preprocessing/#src.preprocessing.convert_images_to_numpy_format--example-usage","text":">>> from src.preprocessing import convert_images_to_numpy_format >>> tumor_folder = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/raw/tumor' ) >>> convert_images_to_numpy_format ( tumor_folder ) >>> output : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/tumor' ) Source code in src/preprocessing.py def convert_images_to_numpy_format ( input_directory : pathlib . PosixPath , ) -> pathlib . PosixPath : \"\"\" wrapper function that reduces images in the input folder and converts content of folder to numpy format in parallel (faster) Parameters ---------- **input_directory**: *(pathlib.PosixPath object)* relative path to the images which should be preprocessed Returns ------ **output_directory**: *(pathlib.PosixPath object)* relative path for the folder with preprocessed results (tranferred and converted images) Example Usage -------------- ```python >>>from src.preprocessing import convert_images_to_numpy_format >>>tumor_folder = Path('ppdm/data/5IT_DUMMY_STUDY/source/raw/tumor') >>>convert_images_to_numpy_format(tumor_folder) >>>output: Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/tumor') ``` \"\"\" output_directory = create_preprocessing_relat_directory ( input_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) directory_ok = check_content_of_two_directories ( input_directory , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) parallel_create_numpy_formats_of_input_img = partial ( _create_numpy_formats_of_input_img , output_directory = output_directory , ) parallel ( parallel_create_numpy_formats_of_input_img , sorted ( list ( input_directory . glob ( \"*.tiff\" ))), n_workers = 12 , progress = True , threadpool = True , ) return output_directory","title":"Example Usage"},{"location":"Modules/preprocessing/#src.preprocessing.data_preprocessing_wrapper","text":"High level function that covers preprocessing for all data (blood vessels, tumors, virus). It used the convert_images_to_numpy_format function and applies to each channel (blood vessels, tumors, virus). If you want to preprocess just one channel (e.g. only tumors) use the convert_images_to_numpy_format function.","title":"data_preprocessing_wrapper()"},{"location":"Modules/preprocessing/#src.preprocessing.data_preprocessing_wrapper--parameters","text":"data : (dict) containing keys (names of the channels) and values (relative paths to it).","title":"Parameters"},{"location":"Modules/preprocessing/#src.preprocessing.data_preprocessing_wrapper--returns","text":"preprocessed_data : *(dict) outputs three relative paths for the folders with preprocessed results (tranferred and converted images) Results are stored on the disk.","title":"Returns"},{"location":"Modules/preprocessing/#src.preprocessing.data_preprocessing_wrapper--example-usage","text":">>> from src.preprocessing import data_preprocessing_wrapper >>> study_paths = { 'vessel' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/raw/vessel' ), 'tumor' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/raw/tumor' ), 'virus' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/raw/virus' )} >>> data_preprocessing_wrapper ( study_paths ) >>> output : defaultdict ( < function src . utils . nested_dict () > , { 'vessel' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), 'tumor' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/tumor' ), 'virus' : Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/virus' )}) Source code in src/preprocessing.py @log_step def data_preprocessing_wrapper ( data : dict ) -> dict : \"\"\" High level function that covers preprocessing for all data (blood vessels, tumors, virus). It used the convert_images_to_numpy_format function and applies to each channel (blood vessels, tumors, virus). If you want to preprocess just one channel (e.g. only tumors) use the convert_images_to_numpy_format function. Parameters ---------- **data**: *(dict)* containing keys (names of the channels) and values (relative paths to it). Returns ------ **preprocessed_data**: *(dict) outputs three relative paths for the folders with preprocessed results (tranferred and converted images) Results are stored on the disk. Example Usage -------------- ```python >>>from src.preprocessing import data_preprocessing_wrapper >>>study_paths = {'vessel': Path('ppdm/data/5IT_DUMMY_STUDY/source/raw/vessel'), 'tumor': Path('ppdm/data/5IT_DUMMY_STUDY/source/raw/tumor'), 'virus': Path('ppdm/data/5IT_DUMMY_STUDY/source/raw/virus')} >>>data_preprocessing_wrapper(study_paths) >>>output: defaultdict(<function src.utils.nested_dict()>, {'vessel': Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), 'tumor': Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/tumor'), 'virus': Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/virus')}) ``` \"\"\" preprocessed_data = nested_dict () out_path = convert_images_to_numpy_format ( data [ \"vessel\" ]) preprocessed_data [ \"vessel\" ] = out_path # tumor out_path = convert_images_to_numpy_format ( data [ \"tumor\" ]) preprocessed_data [ \"tumor\" ] = out_path # virus out_path = convert_images_to_numpy_format ( data [ \"virus\" ]) preprocessed_data [ \"virus\" ] = out_path return preprocessed_data This step will transform and downsize input data ( tumor , vessel , virus ) .tiff files into python's numpy array files, which will be saved to the output path directory. The new folder structure will look as follows: ppdm \u2514\u2500 data \u2514\u2500 5IT_STUDY \u2514\u2500 config.json \u2514\u2500 source \u2514\u2500raw \u2502 \u2514\u2500tumor \u2502 \u2502 \u2514\u2500 5IT-4X_Ch2_z0300.tiff \u2502 \u2502 \u2514\u2500 ... \u2502 \u2502 \u2514\u2500 5IT-4X_Ch2_z1300.tiff \u2502 \u251c\u2500vessel \u2502 \u2502 \u2514\u2500 5IT-4X_Ch3_z0300.tiff \u2502 \u2502 \u2514\u2500 ... \u2502 \u2502 \u2514\u2500 5IT-4X_Ch3_z1300.tiff \u2502 \u2502\u2500virus \u2502 \u2514\u2500 5IT-4X_Ch1_z0300.tiff \u2502 \u2514\u2500 ... \u2502 \u2514\u25005IT-4X_Ch1_z1300.tiff ------------\u2502------------------------------------------------------- \u2514\u2500transformed \u2514\u2500 np_and_resized \u2514\u2500tumor \u2502 \u2514\u2500 5IT-4X_Ch2_z0300.np \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch2_z1300.np \u251c\u2500vessel \u2502 \u2514\u2500 5IT-4X_Ch3_z0300.np \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 5IT-4X_Ch3_z1300.np \u2502\u2500virus \u2514\u2500 5IT-4X_Ch1_z0300.np \u2514\u2500 ... \u2514\u25005IT-4X_Ch1_z1300.np","title":"Example Usage"},{"location":"Modules/profiles/","text":"Profiles Profiles module: Computing and Vizualizing final profile This module consists of two main functions: calculate_profile vizualize_profile calculate_profile ( viral_intensity , distance_from_blood_vessel , tumor_mask , pixel_to_microns = 4 , force_overwrite = True ) High level function that calculates profile of viral intensity binned by distance from blood vessel for tumor area. It requires Virus channel, Distance Transform of blood vessels and Tumor masks in order to aggregate the final profile. Parameters viral_intensity : (pathlib.PosixPath) Relative path to the transformed (resized and numpy format) of Virus channel. distance_from_blood_vessel : (pathlib.PosixPath) Relative path to the distance transform arrays calculated from the segmented blood vessels masks. tumor_mask : (pathlib.PosixPath) Relative path to the preprocessed tumor masks. It can also be multi-color masks for the core-periphery separation. pixel_to_microns (float) Ration between pixels and microns. (When Images multiplied by 4) force_overwrite (bool) if true profiles will be recalculated from scratch even if it has been calculated before. Returns Dict : *(dict) dictionary containing the results in a nested form. One Key are profiles calculated for entire tumor, another are calculated only for core etc . Example Usage >>> from src.profiles import calculate_profile >>> viral_intensity = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/virus' ) >>> distance_from_blood_vessel = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/distance_transform/vessel/distance_tranform___segment___unet___model_file-model' ) >>> tumor_mask = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/segmentation_postprocessing/tumor/postprocess_masks___split_tumor_into_core_and_periphery___segment___thresholding___method-th_triangle' ) >>> pixel_to_microns = 4 >>> force_overwrite = False >>> profiles = calculate_profile ( viral_intensity = viral_intensity , distance_from_blood_vessel = distance_from_blood_vessel , tumor_mask = tumor_mask , pixel_to_microns = pixel_to_microns , force_overwrite = force_overwrite ) >>> profiles >>> output : { 'all' : distance virus 4.000000 721.423291 6.648681 721.463944 10.449533 720.214587 14.169995 716.011269 18.046974 715.064906 Source code in src/profiles.py @log_step def calculate_profile ( viral_intensity : pathlib . PosixPath , distance_from_blood_vessel : pathlib . PosixPath , tumor_mask : pathlib . PosixPath , pixel_to_microns : float = 4 , force_overwrite : bool = True , ) -> Dict : \"\"\" High level function that calculates profile of viral intensity binned by distance from blood vessel for tumor area. It requires *Virus* channel, *Distance Transform of blood vessels* and *Tumor masks* in order to aggregate the final profile. Parameters ---------- **viral_intensity**: *(pathlib.PosixPath)* Relative path to the transformed (resized and numpy format) of Virus channel. **distance_from_blood_vessel**: *(pathlib.PosixPath)* Relative path to the distance transform arrays calculated from the segmented blood vessels masks. **tumor_mask**: *(pathlib.PosixPath)* Relative path to the preprocessed tumor masks. It can also be multi-color masks for the core-periphery separation. **pixel_to_microns** *(float)* Ration between pixels and microns. (When Images multiplied by 4) **force_overwrite** *(bool)* if true profiles will be recalculated from scratch even if it has been calculated before. Returns ------ **Dict**: *(dict) dictionary containing the results in a nested form. One Key are profiles calculated for entire tumor, another are calculated only for core etc . Example Usage -------------- ```python >>>from src.profiles import calculate_profile >>>viral_intensity = Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/virus') >>>distance_from_blood_vessel = Path('ppdm/data/5IT_DUMMY_STUDY/results/distance_transform/vessel/distance_tranform___segment___unet___model_file-model') >>>tumor_mask = Path('ppdm/data/5IT_DUMMY_STUDY/results/segmentation_postprocessing/tumor/postprocess_masks___split_tumor_into_core_and_periphery___segment___thresholding___method-th_triangle') >>>pixel_to_microns = 4 >>>force_overwrite=False >>>profiles = calculate_profile( viral_intensity = viral_intensity, distance_from_blood_vessel = distance_from_blood_vessel, tumor_mask = tumor_mask, pixel_to_microns = pixel_to_microns, force_overwrite=force_overwrite) >>>profiles >>>output: {'all': distance virus 4.000000 721.423291 6.648681 721.463944 10.449533 720.214587 14.169995 716.011269 18.046974 715.064906 ``` \"\"\" module_results_path = \"results/profiles\" virus_paths = sorted ( list ( Path ( viral_intensity ) . glob ( \"*\" ))) distance_paths = sorted ( list ( Path ( distance_from_blood_vessel ) . glob ( \"*\" ))) tumor_mask_paths = sorted ( list ( Path ( tumor_mask ) . glob ( \"*\" ))) output_directory = create_relative_path ( viral_intensity , module_results_path , f \" { distance_from_blood_vessel . parts [ - 1 ] } / { tumor_mask . parts [ - 1 ] } /pixel_to_microns- { pixel_to_microns } \" , ) output_file_path = output_directory / \"profiles.pickle\" if output_file_path . exists () and force_overwrite is False : with open ( output_file_path , \"rb\" ) as file : profiles = pickle . load ( file ) else : inputs = [ { \"distance\" : distance , \"virus\" : virus , \"tumor\" : tumor } for distance , virus , tumor in zip ( distance_paths , virus_paths , tumor_mask_paths ) ] profiles_list = parallel ( _profile_data_from_one_layer_wrapper , inputs , n_workers = 12 , progress = True , threadpool = True , ) profiles = _aggregate_profiles ( profiles_list ) # convert distance to microns for key in profiles : profiles [ key ] = rescale_column_pandas ( profiles [ key ], \"distance\" , pixel_to_microns ) profiles [ key ] . reset_index ( drop = True , inplace = True ) # save results with open ( output_file_path , \"wb\" ) as file : pickle . dump ( profiles , file ) return profiles vizualize_profile ( data_frame , distance_threshold_microns = 100 , ylim_bottom = None , plot = True ) Function that vizualizes the aggregated profiles (output of the calculate_profile function) Parameters data_frame : (pd.DataFrame) Dataframe with aggregated profiles to use for the plot distance_threshold_microns : (int) Maximum distance (microns) on the x-axis. ylim_bottom : (float) Minumum values to use in the y-axis. plot (bool) Whether the profile should be plotted or not (usefull when plotting multiple profiles in one cell. Otherwise can be ignored) Returns None Example Usage >>> from src.profiles import vizualize_profile >>> profile_all = vizualize_profile ( profiles [ \"all\" ]) >>> profile_core = vizualize_profile ( profiles [ \"core\" ]) >>> profile_periphery = vizualize_profile ( profiles [ \"periphery\" ]) Source code in src/profiles.py def vizualize_profile ( data_frame : pd . DataFrame , distance_threshold_microns : int = 100 , ylim_bottom : float = None , plot : bool = True , ) -> plt . figure : \"\"\" Function that vizualizes the aggregated profiles (output of the calculate_profile function) Parameters ---------- **data_frame**: *(pd.DataFrame)* Dataframe with aggregated profiles to use for the plot **distance_threshold_microns**: *(int)* Maximum distance (microns) on the x-axis. **ylim_bottom**: *(float)* Minumum values to use in the y-axis. **plot** *(bool)* Whether the profile should be plotted or not (usefull when plotting multiple profiles in one cell. Otherwise can be ignored) Returns ------ None Example Usage -------------- ```python >>>from src.profiles import vizualize_profile >>>profile_all = vizualize_profile(profiles[\"all\"]) >>>profile_core = vizualize_profile(profiles[\"core\"]) >>>profile_periphery = vizualize_profile(profiles[\"periphery\"]) ``` \"\"\" profile = data_frame . copy () profile_subset = profile [ profile . distance <= distance_threshold_microns ] # log as figure fig = plt . figure ( figsize = ( 6 , 6 )) plt . rcParams . update ({ \"font.size\" : 12 }) plt . plot ( profile_subset [ \"distance\" ], profile_subset [ \"virus\" ], color = \"black\" ) plt . xlabel ( \"Distance from blood vessel [microns]\" , size = 14 ) plt . ylabel ( \"Viral Intensity\" , size = 14 ) if ylim_bottom is not None : plt . ylim ( bottom = ylim_bottom ) if plot : plt . show () plt . close () return fig","title":"Profiles"},{"location":"Modules/profiles/#profiles","text":"","title":"Profiles"},{"location":"Modules/profiles/#src.profiles--profiles-module","text":"","title":"Profiles module:"},{"location":"Modules/profiles/#src.profiles--computing-and-vizualizing-final-profile","text":"","title":"Computing and Vizualizing final profile"},{"location":"Modules/profiles/#src.profiles--this-module-consists-of-two-main-functions","text":"calculate_profile vizualize_profile","title":"This module consists of two main functions:"},{"location":"Modules/profiles/#src.profiles.calculate_profile","text":"High level function that calculates profile of viral intensity binned by distance from blood vessel for tumor area. It requires Virus channel, Distance Transform of blood vessels and Tumor masks in order to aggregate the final profile.","title":"calculate_profile()"},{"location":"Modules/profiles/#src.profiles.calculate_profile--parameters","text":"viral_intensity : (pathlib.PosixPath) Relative path to the transformed (resized and numpy format) of Virus channel. distance_from_blood_vessel : (pathlib.PosixPath) Relative path to the distance transform arrays calculated from the segmented blood vessels masks. tumor_mask : (pathlib.PosixPath) Relative path to the preprocessed tumor masks. It can also be multi-color masks for the core-periphery separation. pixel_to_microns (float) Ration between pixels and microns. (When Images multiplied by 4) force_overwrite (bool) if true profiles will be recalculated from scratch even if it has been calculated before.","title":"Parameters"},{"location":"Modules/profiles/#src.profiles.calculate_profile--returns","text":"Dict : *(dict) dictionary containing the results in a nested form. One Key are profiles calculated for entire tumor, another are calculated only for core etc .","title":"Returns"},{"location":"Modules/profiles/#src.profiles.calculate_profile--example-usage","text":">>> from src.profiles import calculate_profile >>> viral_intensity = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/virus' ) >>> distance_from_blood_vessel = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/distance_transform/vessel/distance_tranform___segment___unet___model_file-model' ) >>> tumor_mask = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/segmentation_postprocessing/tumor/postprocess_masks___split_tumor_into_core_and_periphery___segment___thresholding___method-th_triangle' ) >>> pixel_to_microns = 4 >>> force_overwrite = False >>> profiles = calculate_profile ( viral_intensity = viral_intensity , distance_from_blood_vessel = distance_from_blood_vessel , tumor_mask = tumor_mask , pixel_to_microns = pixel_to_microns , force_overwrite = force_overwrite ) >>> profiles >>> output : { 'all' : distance virus 4.000000 721.423291 6.648681 721.463944 10.449533 720.214587 14.169995 716.011269 18.046974 715.064906 Source code in src/profiles.py @log_step def calculate_profile ( viral_intensity : pathlib . PosixPath , distance_from_blood_vessel : pathlib . PosixPath , tumor_mask : pathlib . PosixPath , pixel_to_microns : float = 4 , force_overwrite : bool = True , ) -> Dict : \"\"\" High level function that calculates profile of viral intensity binned by distance from blood vessel for tumor area. It requires *Virus* channel, *Distance Transform of blood vessels* and *Tumor masks* in order to aggregate the final profile. Parameters ---------- **viral_intensity**: *(pathlib.PosixPath)* Relative path to the transformed (resized and numpy format) of Virus channel. **distance_from_blood_vessel**: *(pathlib.PosixPath)* Relative path to the distance transform arrays calculated from the segmented blood vessels masks. **tumor_mask**: *(pathlib.PosixPath)* Relative path to the preprocessed tumor masks. It can also be multi-color masks for the core-periphery separation. **pixel_to_microns** *(float)* Ration between pixels and microns. (When Images multiplied by 4) **force_overwrite** *(bool)* if true profiles will be recalculated from scratch even if it has been calculated before. Returns ------ **Dict**: *(dict) dictionary containing the results in a nested form. One Key are profiles calculated for entire tumor, another are calculated only for core etc . Example Usage -------------- ```python >>>from src.profiles import calculate_profile >>>viral_intensity = Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/virus') >>>distance_from_blood_vessel = Path('ppdm/data/5IT_DUMMY_STUDY/results/distance_transform/vessel/distance_tranform___segment___unet___model_file-model') >>>tumor_mask = Path('ppdm/data/5IT_DUMMY_STUDY/results/segmentation_postprocessing/tumor/postprocess_masks___split_tumor_into_core_and_periphery___segment___thresholding___method-th_triangle') >>>pixel_to_microns = 4 >>>force_overwrite=False >>>profiles = calculate_profile( viral_intensity = viral_intensity, distance_from_blood_vessel = distance_from_blood_vessel, tumor_mask = tumor_mask, pixel_to_microns = pixel_to_microns, force_overwrite=force_overwrite) >>>profiles >>>output: {'all': distance virus 4.000000 721.423291 6.648681 721.463944 10.449533 720.214587 14.169995 716.011269 18.046974 715.064906 ``` \"\"\" module_results_path = \"results/profiles\" virus_paths = sorted ( list ( Path ( viral_intensity ) . glob ( \"*\" ))) distance_paths = sorted ( list ( Path ( distance_from_blood_vessel ) . glob ( \"*\" ))) tumor_mask_paths = sorted ( list ( Path ( tumor_mask ) . glob ( \"*\" ))) output_directory = create_relative_path ( viral_intensity , module_results_path , f \" { distance_from_blood_vessel . parts [ - 1 ] } / { tumor_mask . parts [ - 1 ] } /pixel_to_microns- { pixel_to_microns } \" , ) output_file_path = output_directory / \"profiles.pickle\" if output_file_path . exists () and force_overwrite is False : with open ( output_file_path , \"rb\" ) as file : profiles = pickle . load ( file ) else : inputs = [ { \"distance\" : distance , \"virus\" : virus , \"tumor\" : tumor } for distance , virus , tumor in zip ( distance_paths , virus_paths , tumor_mask_paths ) ] profiles_list = parallel ( _profile_data_from_one_layer_wrapper , inputs , n_workers = 12 , progress = True , threadpool = True , ) profiles = _aggregate_profiles ( profiles_list ) # convert distance to microns for key in profiles : profiles [ key ] = rescale_column_pandas ( profiles [ key ], \"distance\" , pixel_to_microns ) profiles [ key ] . reset_index ( drop = True , inplace = True ) # save results with open ( output_file_path , \"wb\" ) as file : pickle . dump ( profiles , file ) return profiles","title":"Example Usage"},{"location":"Modules/profiles/#src.profiles.vizualize_profile","text":"Function that vizualizes the aggregated profiles (output of the calculate_profile function)","title":"vizualize_profile()"},{"location":"Modules/profiles/#src.profiles.vizualize_profile--parameters","text":"data_frame : (pd.DataFrame) Dataframe with aggregated profiles to use for the plot distance_threshold_microns : (int) Maximum distance (microns) on the x-axis. ylim_bottom : (float) Minumum values to use in the y-axis. plot (bool) Whether the profile should be plotted or not (usefull when plotting multiple profiles in one cell. Otherwise can be ignored)","title":"Parameters"},{"location":"Modules/profiles/#src.profiles.vizualize_profile--returns","text":"None","title":"Returns"},{"location":"Modules/profiles/#src.profiles.vizualize_profile--example-usage","text":">>> from src.profiles import vizualize_profile >>> profile_all = vizualize_profile ( profiles [ \"all\" ]) >>> profile_core = vizualize_profile ( profiles [ \"core\" ]) >>> profile_periphery = vizualize_profile ( profiles [ \"periphery\" ]) Source code in src/profiles.py def vizualize_profile ( data_frame : pd . DataFrame , distance_threshold_microns : int = 100 , ylim_bottom : float = None , plot : bool = True , ) -> plt . figure : \"\"\" Function that vizualizes the aggregated profiles (output of the calculate_profile function) Parameters ---------- **data_frame**: *(pd.DataFrame)* Dataframe with aggregated profiles to use for the plot **distance_threshold_microns**: *(int)* Maximum distance (microns) on the x-axis. **ylim_bottom**: *(float)* Minumum values to use in the y-axis. **plot** *(bool)* Whether the profile should be plotted or not (usefull when plotting multiple profiles in one cell. Otherwise can be ignored) Returns ------ None Example Usage -------------- ```python >>>from src.profiles import vizualize_profile >>>profile_all = vizualize_profile(profiles[\"all\"]) >>>profile_core = vizualize_profile(profiles[\"core\"]) >>>profile_periphery = vizualize_profile(profiles[\"periphery\"]) ``` \"\"\" profile = data_frame . copy () profile_subset = profile [ profile . distance <= distance_threshold_microns ] # log as figure fig = plt . figure ( figsize = ( 6 , 6 )) plt . rcParams . update ({ \"font.size\" : 12 }) plt . plot ( profile_subset [ \"distance\" ], profile_subset [ \"virus\" ], color = \"black\" ) plt . xlabel ( \"Distance from blood vessel [microns]\" , size = 14 ) plt . ylabel ( \"Viral Intensity\" , size = 14 ) if ylim_bottom is not None : plt . ylim ( bottom = ylim_bottom ) if plot : plt . show () plt . close () return fig","title":"Example Usage"},{"location":"Modules/segmentation/","text":"Segmentation Some segmentation models (random_forest and unet) require a pre-trained model as a function argument. segmentation module: module for blood vessels and tumor boundary segmentation This module consists of four main functions: random_forest segmentation_wrapper thresholding unet random_forest ( data_path , output_directory , model_file ) Function that performs the segmentation of the blood vessels using Random Forest model (if you want to segment e.g. tumors this function can still be used. However, you need to provide a pre-trained model (model_file) that is pretrained for tumors) Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided. Parameters data_path : (pathlib.PosixPath object) relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) output_directory (pathlib.PosixPath object) relative path to a folder where the results should be saved model_file : (pathlib.PosixPath object) relative path to the pre-trained model binary file, which should be used for the segmentation Returns None: Results are stored on the disk. Example Usage >>> from src.segmentation import random_forest >>> random_forest ( data_path = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), output_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/RF/raw/vessel' ), model_file = Path ( 'pretrained_models/rf_model.joblib' ) ) Source code in src/segmentation.py @log_step def random_forest ( data_path : pathlib . PosixPath , output_directory : pathlib . PosixPath , model_file , ) -> None : \"\"\" Function that performs the segmentation of the blood vessels using Random Forest model (if you want to segment e.g. tumors this function can still be used. However, you need to provide a pre-trained model (model_file) that is pretrained for tumors) Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided. Parameters ---------- **data_path**: *(pathlib.PosixPath object)* relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) **output_directory** *(pathlib.PosixPath object)* relative path to a folder where the results should be saved **model_file**: *(pathlib.PosixPath object)* relative path to the pre-trained model binary file, which should be used for the segmentation Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import random_forest >>>random_forest( data_path = Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), output_directory = Path('ppdm/data/5IT_DUMMY_STUDY/RF/raw/vessel'), model_file = Path('pretrained_models/rf_model.joblib') ) ``` \"\"\" loaded_rf = load ( model_file ) segment_rf_partial = partial ( _segment_random_forest , loaded_estimator_to_use = loaded_rf , output_directory = output_directory , ) directory_ok = check_content_of_two_directories ( data_path , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) parallel ( segment_rf_partial , sorted ( list ( data_path . glob ( \"*.npy\" ))), n_workers = 12 , progress = True , threadpool = True , ) segmentation_wrapper ( data_path , method , method_parameters ) This function is a main wrapper for segmentation used within the automated pipeline. It uses functions written in this script and wraps them in this wrapper (one big function). If one want to use segmentations methods in a script, you may use individual segmentation functions -> unet, thresholding, random_forest Parameters data_path : (pathlib.PosixPath object) relative path to the study which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) method : (str) method to use for segmentation (unet, thresholding, random_forest) method_parameters : (Dict) relative path to the pre-trained model binary file, which should be used for the segmentation device : (str) graphic card which should be used (relevant only to cloud computing environment). For local code the graphic card is selected automatically based on the PC hardware. Returns None: Results are stored on the disk. Example Usage >>> from src.segmentation import unet >>> unet ( input_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), output_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/DOCUMENTATION/raw/vessel' ), model_file = Path ( 'pretrained_models/unet_train_full_5IT_7IV_8IV.pt' ) ) Source code in src/segmentation.py @log_step def segmentation_wrapper ( data_path : pathlib . PosixPath , method : str , method_parameters : Dict ) -> pathlib . PosixPath : \"\"\" This function is a main wrapper for segmentation used within the automated pipeline. It uses functions written in this script and wraps them in this wrapper (one big function). If one want to use segmentations methods in a script, you may use individual segmentation functions -> unet, thresholding, random_forest Parameters ---------- **data_path** : *(pathlib.PosixPath object)* relative path to the study which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) **method** : *(str)* method to use for segmentation (unet, thresholding, random_forest) **method_parameters**: *(Dict)* relative path to the pre-trained model binary file, which should be used for the segmentation **device**: *(str)* graphic card which should be used (relevant only to cloud computing environment). For local code the graphic card is selected automatically based on the PC hardware. Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import unet >>>unet( input_directory = Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), output_directory = Path('ppdm/data/5IT_DUMMY_STUDY/DOCUMENTATION/raw/vessel'), model_file = Path('pretrained_models/unet_train_full_5IT_7IV_8IV.pt') ) ``` \"\"\" module_results_path = \"results/segmentation\" module_name = \"segment\" segmentation_function = _select_method ( method ) parameters_as_string = join_keys_and_values_to_list ( method_parameters ) output_folder_name = join_to_string ( [ module_name , method , * parameters_as_string ] ) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name ) directory_ok = check_content_of_two_directories ( data_path , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) segmentation_function ( data_path , output_directory , ** method_parameters ) return output_directory thresholding ( data_path , output_directory , method , mask_path = None ) Function that performs the segmentation using thresholding. This function can be used for any channel (e.g. vessels, tumors) as it calculates \"optimal\" threshold value automatically. Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided. Parameters data_path : (pathlib.PosixPath object) relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) output_directory (pathlib.PosixPath object) relative path to a folder where the results should be saved method : (str) segmentation method to be used. Supported methods are \"th_otsu\", \"th_triangle\", \"th_yen\". ANOTHER OPTIONAL PARAMETERS: mask_path : (pathlib.PosixPath object) relative path for the binary mask to be used to filter the area of interest on which the segmentation will be performed. Returns None: Results are stored on the disk. Example Usage >>> from src.segmentation import thresholding >>> thresholding ( data_path = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), output_directory = Path ( 'msd_projects/ppdm/data/5IT_DUMMY_STUDY/THRES/raw/vessel' ), method = \"th_otsu\" ) Source code in src/segmentation.py @log_step def thresholding ( data_path : pathlib . PosixPath , output_directory : pathlib . PosixPath , method : str , mask_path : pathlib . PosixPath = None , ) -> None : \"\"\" Function that performs the segmentation using thresholding. This function can be used for any channel (e.g. vessels, tumors) as it calculates \"optimal\" threshold value automatically. Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided. Parameters ---------- **data_path**: *(pathlib.PosixPath object)* relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) **output_directory** *(pathlib.PosixPath object)* relative path to a folder where the results should be saved **method**: *(str)* segmentation method to be used. Supported methods are \"th_otsu\", \"th_triangle\", \"th_yen\". ANOTHER OPTIONAL PARAMETERS: **mask_path**: *(pathlib.PosixPath object)* relative path for the binary mask to be used to filter the area of interest on which the segmentation will be performed. Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import thresholding >>>thresholding( data_path = Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), output_directory = Path('msd_projects/ppdm/data/5IT_DUMMY_STUDY/THRES/raw/vessel'), method = \"th_otsu\" ) ``` \"\"\" supported_methods = [ \"th_otsu\" , \"th_triangle\" , \"th_yen\" ] if method not in supported_methods : raise ValueError ( f \"thresholding method must be either in : { supported_methods } , you have provided: { method } \" ) threshold_value = _calculate_threshold_value ( data_path , method , mask_path ) print ( f \"threshold { method } value: { threshold_value } \" ) _segment_and_save_masks_threshold ( data_path , threshold_value , output_directory ) unet ( input_directory , output_directory , model_file , device = None ) Function that performs the segmentation of the blood vessels (if you want to segment e.g. tumors this function can still be used. However, you need to provide a pre-trained model (model_file) that is pretrained for tumors) Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided. Parameters input_directory : (pathlib.PosixPath object) relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) output_directory : (pathlib.PosixPath object) relative path to a folder where the results should be saved model_file : (pathlib.PosixPath object) relative path to the pre-trained model binary file, which should be used for the segmentation device : (str) graphic card which should be used (relevant only to cloud computing environment). For local code the graphic card is selected automatically based on the PC hardware. Returns None: Results are stored on the disk. Example Usage >>> from src.segmentation import unet >>> unet ( input_directory = Path ( 'msd_projects/ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), output_directory = Path ( 'msd_projects/ppdm/data/5IT_DUMMY_STUDY/DOCUMENTATION/raw/vessel' ), model_file = Path ( 'pretrained_models/unet_train_full_5IT_7IV_8IV.pt' ) ) Source code in src/segmentation.py @log_step def unet ( input_directory : pathlib . PosixPath , output_directory : pathlib . PosixPath , model_file : pathlib . PosixPath , device : str = None , ) -> None : \"\"\" Function that performs the segmentation of the blood vessels (if you want to segment e.g. tumors this function can still be used. However, you need to provide a pre-trained model (model_file) that is pretrained for tumors) Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided. Parameters ---------- **input_directory** : *(pathlib.PosixPath object)* relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) **output_directory** : *(pathlib.PosixPath object)* relative path to a folder where the results should be saved **model_file**: *(pathlib.PosixPath object)* relative path to the pre-trained model binary file, which should be used for the segmentation **device**: *(str)* graphic card which should be used (relevant only to cloud computing environment). For local code the graphic card is selected automatically based on the PC hardware. Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import unet >>>unet( input_directory = Path('msd_projects/ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), output_directory = Path('msd_projects/ppdm/data/5IT_DUMMY_STUDY/DOCUMENTATION/raw/vessel'), model_file = Path('pretrained_models/unet_train_full_5IT_7IV_8IV.pt') ) ``` \"\"\" input_directory_paths = sorted ( list ( input_directory . glob ( \"*.npy\" ))) directory_ok = check_content_of_two_directories ( input_directory , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) if device is None : device = f \"cuda: { select_avail_gpu () } \" model = torch . load ( model_file ) model = model . to ( device ) . eval () for file in tqdm ( input_directory_paths ): image = np . load ( file ) # convert 16 bit images to 8bits if image . dtype == \"uint16\" : image = image / 65535 * 255 image = image . astype ( np . uint8 ) image = np . expand_dims ( image , axis = 0 ) # split images to tiles tiler = VolumeSlicer ( image . shape , voxel_size = ( image . shape [ 0 ], 512 , 512 ), voxel_step = ( image . shape [ 0 ], 512 , 512 ), ) tiles = tiler . split ( image ) tiles_processed = _unet_runner ( tiles , model , device ) # merge tiles back to one image tiles_stiched = tiler . merge ( tiles_processed ) mask = tiles_stiched [ 0 , :, :] # save mask save_path_mask = output_directory / file . name output_directory . mkdir ( parents = True , exist_ok = True ) np . save ( save_path_mask , mask . astype ( np . uint8 ))","title":"Segmentation"},{"location":"Modules/segmentation/#segmentation","text":"Some segmentation models (random_forest and unet) require a pre-trained model as a function argument.","title":"Segmentation"},{"location":"Modules/segmentation/#src.segmentation--segmentation-module","text":"","title":"segmentation module:"},{"location":"Modules/segmentation/#src.segmentation--module-for-blood-vessels-and-tumor-boundary-segmentation","text":"","title":"module for blood vessels and tumor boundary segmentation"},{"location":"Modules/segmentation/#src.segmentation--this-module-consists-of-four-main-functions","text":"random_forest segmentation_wrapper thresholding unet","title":"This module consists of four main functions:"},{"location":"Modules/segmentation/#src.segmentation.random_forest","text":"Function that performs the segmentation of the blood vessels using Random Forest model (if you want to segment e.g. tumors this function can still be used. However, you need to provide a pre-trained model (model_file) that is pretrained for tumors) Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided.","title":"random_forest()"},{"location":"Modules/segmentation/#src.segmentation.random_forest--parameters","text":"data_path : (pathlib.PosixPath object) relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) output_directory (pathlib.PosixPath object) relative path to a folder where the results should be saved model_file : (pathlib.PosixPath object) relative path to the pre-trained model binary file, which should be used for the segmentation","title":"Parameters"},{"location":"Modules/segmentation/#src.segmentation.random_forest--returns","text":"None: Results are stored on the disk.","title":"Returns"},{"location":"Modules/segmentation/#src.segmentation.random_forest--example-usage","text":">>> from src.segmentation import random_forest >>> random_forest ( data_path = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), output_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/RF/raw/vessel' ), model_file = Path ( 'pretrained_models/rf_model.joblib' ) ) Source code in src/segmentation.py @log_step def random_forest ( data_path : pathlib . PosixPath , output_directory : pathlib . PosixPath , model_file , ) -> None : \"\"\" Function that performs the segmentation of the blood vessels using Random Forest model (if you want to segment e.g. tumors this function can still be used. However, you need to provide a pre-trained model (model_file) that is pretrained for tumors) Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided. Parameters ---------- **data_path**: *(pathlib.PosixPath object)* relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) **output_directory** *(pathlib.PosixPath object)* relative path to a folder where the results should be saved **model_file**: *(pathlib.PosixPath object)* relative path to the pre-trained model binary file, which should be used for the segmentation Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import random_forest >>>random_forest( data_path = Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), output_directory = Path('ppdm/data/5IT_DUMMY_STUDY/RF/raw/vessel'), model_file = Path('pretrained_models/rf_model.joblib') ) ``` \"\"\" loaded_rf = load ( model_file ) segment_rf_partial = partial ( _segment_random_forest , loaded_estimator_to_use = loaded_rf , output_directory = output_directory , ) directory_ok = check_content_of_two_directories ( data_path , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) parallel ( segment_rf_partial , sorted ( list ( data_path . glob ( \"*.npy\" ))), n_workers = 12 , progress = True , threadpool = True , )","title":"Example Usage"},{"location":"Modules/segmentation/#src.segmentation.segmentation_wrapper","text":"This function is a main wrapper for segmentation used within the automated pipeline. It uses functions written in this script and wraps them in this wrapper (one big function). If one want to use segmentations methods in a script, you may use individual segmentation functions -> unet, thresholding, random_forest","title":"segmentation_wrapper()"},{"location":"Modules/segmentation/#src.segmentation.segmentation_wrapper--parameters","text":"data_path : (pathlib.PosixPath object) relative path to the study which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) method : (str) method to use for segmentation (unet, thresholding, random_forest) method_parameters : (Dict) relative path to the pre-trained model binary file, which should be used for the segmentation device : (str) graphic card which should be used (relevant only to cloud computing environment). For local code the graphic card is selected automatically based on the PC hardware.","title":"Parameters"},{"location":"Modules/segmentation/#src.segmentation.segmentation_wrapper--returns","text":"None: Results are stored on the disk.","title":"Returns"},{"location":"Modules/segmentation/#src.segmentation.segmentation_wrapper--example-usage","text":">>> from src.segmentation import unet >>> unet ( input_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), output_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/DOCUMENTATION/raw/vessel' ), model_file = Path ( 'pretrained_models/unet_train_full_5IT_7IV_8IV.pt' ) ) Source code in src/segmentation.py @log_step def segmentation_wrapper ( data_path : pathlib . PosixPath , method : str , method_parameters : Dict ) -> pathlib . PosixPath : \"\"\" This function is a main wrapper for segmentation used within the automated pipeline. It uses functions written in this script and wraps them in this wrapper (one big function). If one want to use segmentations methods in a script, you may use individual segmentation functions -> unet, thresholding, random_forest Parameters ---------- **data_path** : *(pathlib.PosixPath object)* relative path to the study which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) **method** : *(str)* method to use for segmentation (unet, thresholding, random_forest) **method_parameters**: *(Dict)* relative path to the pre-trained model binary file, which should be used for the segmentation **device**: *(str)* graphic card which should be used (relevant only to cloud computing environment). For local code the graphic card is selected automatically based on the PC hardware. Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import unet >>>unet( input_directory = Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), output_directory = Path('ppdm/data/5IT_DUMMY_STUDY/DOCUMENTATION/raw/vessel'), model_file = Path('pretrained_models/unet_train_full_5IT_7IV_8IV.pt') ) ``` \"\"\" module_results_path = \"results/segmentation\" module_name = \"segment\" segmentation_function = _select_method ( method ) parameters_as_string = join_keys_and_values_to_list ( method_parameters ) output_folder_name = join_to_string ( [ module_name , method , * parameters_as_string ] ) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name ) directory_ok = check_content_of_two_directories ( data_path , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) segmentation_function ( data_path , output_directory , ** method_parameters ) return output_directory","title":"Example Usage"},{"location":"Modules/segmentation/#src.segmentation.thresholding","text":"Function that performs the segmentation using thresholding. This function can be used for any channel (e.g. vessels, tumors) as it calculates \"optimal\" threshold value automatically. Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided.","title":"thresholding()"},{"location":"Modules/segmentation/#src.segmentation.thresholding--parameters","text":"data_path : (pathlib.PosixPath object) relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) output_directory (pathlib.PosixPath object) relative path to a folder where the results should be saved method : (str) segmentation method to be used. Supported methods are \"th_otsu\", \"th_triangle\", \"th_yen\". ANOTHER OPTIONAL PARAMETERS: mask_path : (pathlib.PosixPath object) relative path for the binary mask to be used to filter the area of interest on which the segmentation will be performed.","title":"Parameters"},{"location":"Modules/segmentation/#src.segmentation.thresholding--returns","text":"None: Results are stored on the disk.","title":"Returns"},{"location":"Modules/segmentation/#src.segmentation.thresholding--example-usage","text":">>> from src.segmentation import thresholding >>> thresholding ( data_path = Path ( 'ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), output_directory = Path ( 'msd_projects/ppdm/data/5IT_DUMMY_STUDY/THRES/raw/vessel' ), method = \"th_otsu\" ) Source code in src/segmentation.py @log_step def thresholding ( data_path : pathlib . PosixPath , output_directory : pathlib . PosixPath , method : str , mask_path : pathlib . PosixPath = None , ) -> None : \"\"\" Function that performs the segmentation using thresholding. This function can be used for any channel (e.g. vessels, tumors) as it calculates \"optimal\" threshold value automatically. Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided. Parameters ---------- **data_path**: *(pathlib.PosixPath object)* relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) **output_directory** *(pathlib.PosixPath object)* relative path to a folder where the results should be saved **method**: *(str)* segmentation method to be used. Supported methods are \"th_otsu\", \"th_triangle\", \"th_yen\". ANOTHER OPTIONAL PARAMETERS: **mask_path**: *(pathlib.PosixPath object)* relative path for the binary mask to be used to filter the area of interest on which the segmentation will be performed. Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import thresholding >>>thresholding( data_path = Path('ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), output_directory = Path('msd_projects/ppdm/data/5IT_DUMMY_STUDY/THRES/raw/vessel'), method = \"th_otsu\" ) ``` \"\"\" supported_methods = [ \"th_otsu\" , \"th_triangle\" , \"th_yen\" ] if method not in supported_methods : raise ValueError ( f \"thresholding method must be either in : { supported_methods } , you have provided: { method } \" ) threshold_value = _calculate_threshold_value ( data_path , method , mask_path ) print ( f \"threshold { method } value: { threshold_value } \" ) _segment_and_save_masks_threshold ( data_path , threshold_value , output_directory )","title":"Example Usage"},{"location":"Modules/segmentation/#src.segmentation.unet","text":"Function that performs the segmentation of the blood vessels (if you want to segment e.g. tumors this function can still be used. However, you need to provide a pre-trained model (model_file) that is pretrained for tumors) Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided.","title":"unet()"},{"location":"Modules/segmentation/#src.segmentation.unet--parameters","text":"input_directory : (pathlib.PosixPath object) relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) output_directory : (pathlib.PosixPath object) relative path to a folder where the results should be saved model_file : (pathlib.PosixPath object) relative path to the pre-trained model binary file, which should be used for the segmentation device : (str) graphic card which should be used (relevant only to cloud computing environment). For local code the graphic card is selected automatically based on the PC hardware.","title":"Parameters"},{"location":"Modules/segmentation/#src.segmentation.unet--returns","text":"None: Results are stored on the disk.","title":"Returns"},{"location":"Modules/segmentation/#src.segmentation.unet--example-usage","text":">>> from src.segmentation import unet >>> unet ( input_directory = Path ( 'msd_projects/ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel' ), output_directory = Path ( 'msd_projects/ppdm/data/5IT_DUMMY_STUDY/DOCUMENTATION/raw/vessel' ), model_file = Path ( 'pretrained_models/unet_train_full_5IT_7IV_8IV.pt' ) ) Source code in src/segmentation.py @log_step def unet ( input_directory : pathlib . PosixPath , output_directory : pathlib . PosixPath , model_file : pathlib . PosixPath , device : str = None , ) -> None : \"\"\" Function that performs the segmentation of the blood vessels (if you want to segment e.g. tumors this function can still be used. However, you need to provide a pre-trained model (model_file) that is pretrained for tumors) Function performs the segmentation and saves the results (individual segmented images) to the output_directory path provided. Parameters ---------- **input_directory** : *(pathlib.PosixPath object)* relative path to the images which should be segmented (notice that we work with preprocessed images which are in npy for and not in .tiff format) **output_directory** : *(pathlib.PosixPath object)* relative path to a folder where the results should be saved **model_file**: *(pathlib.PosixPath object)* relative path to the pre-trained model binary file, which should be used for the segmentation **device**: *(str)* graphic card which should be used (relevant only to cloud computing environment). For local code the graphic card is selected automatically based on the PC hardware. Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import unet >>>unet( input_directory = Path('msd_projects/ppdm/data/5IT_DUMMY_STUDY/source/transformed/np_and_resized/vessel'), output_directory = Path('msd_projects/ppdm/data/5IT_DUMMY_STUDY/DOCUMENTATION/raw/vessel'), model_file = Path('pretrained_models/unet_train_full_5IT_7IV_8IV.pt') ) ``` \"\"\" input_directory_paths = sorted ( list ( input_directory . glob ( \"*.npy\" ))) directory_ok = check_content_of_two_directories ( input_directory , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) if device is None : device = f \"cuda: { select_avail_gpu () } \" model = torch . load ( model_file ) model = model . to ( device ) . eval () for file in tqdm ( input_directory_paths ): image = np . load ( file ) # convert 16 bit images to 8bits if image . dtype == \"uint16\" : image = image / 65535 * 255 image = image . astype ( np . uint8 ) image = np . expand_dims ( image , axis = 0 ) # split images to tiles tiler = VolumeSlicer ( image . shape , voxel_size = ( image . shape [ 0 ], 512 , 512 ), voxel_step = ( image . shape [ 0 ], 512 , 512 ), ) tiles = tiler . split ( image ) tiles_processed = _unet_runner ( tiles , model , device ) # merge tiles back to one image tiles_stiched = tiler . merge ( tiles_processed ) mask = tiles_stiched [ 0 , :, :] # save mask save_path_mask = output_directory / file . name output_directory . mkdir ( parents = True , exist_ok = True ) np . save ( save_path_mask , mask . astype ( np . uint8 ))","title":"Example Usage"},{"location":"Modules/segmentation_postprocessing/","text":"Segmentation Postprocessing segmentation postprocessing module: module for certain masks postprocessing operation Implemented Operations: Reducing Tumor Borders and Filling holes TODO: is it discussed in the paper? This module consists of two main functions: postprocess_masks split_tumor_into_core_and_periphery postprocess_masks ( data_path , method , method_parameters ) Wrapper function, which handles the masks postprocessing within the pipeline. For now only used for the tumor masks postprocessing to split masks into core and periphery regions. Parameters data_path : (pathlib.PosixPath) relative path to the segmented masks. method : (str) function which should be applied in order to postprocess masks. Thus far we only use 'split_tumor_into_core_and_periphery' function. method_parameters : (Dict) parameters to use for the function applied in method argument. Returns output_directory (pathlib.PosixPath) : Path where the results have been stored on the disk to. Example Usage >>> from src.segmentation import postprocess_masks >>> postprocess_masks ( input_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/segmentation/tumor/segment___thresholding___method-th_triangle' ), method = 'split_tumor_into_core_and_periphery' , method_parameters = { 'periphery_as_ratio_of_max_distance' : 0.2 } Source code in src/segmentation_postprocessing.py @log_step def postprocess_masks ( data_path : pathlib . PosixPath , method : str , method_parameters : Dict ) -> pathlib . PosixPath : \"\"\" Wrapper function, which handles the masks postprocessing within the pipeline. For now only used for the tumor masks postprocessing to split masks into core and periphery regions. Parameters ---------- **data_path** : *(pathlib.PosixPath)* relative path to the segmented masks. **method**: *(str)* function which should be applied in order to postprocess masks. Thus far we only use 'split_tumor_into_core_and_periphery' function. **method_parameters**: *(Dict)* parameters to use for the function applied in **method** argument. Returns ------ output_directory *(pathlib.PosixPath)*: Path where the results have been stored on the disk to. Example Usage -------------- ```python >>>from src.segmentation import postprocess_masks >>>postprocess_masks( input_directory = Path('ppdm/data/5IT_DUMMY_STUDY/results/segmentation/tumor/segment___thresholding___method-th_triangle'), method = 'split_tumor_into_core_and_periphery', method_parameters = {'periphery_as_ratio_of_max_distance': 0.2} ``` \"\"\" module_results_path = \"results/segmentation_postprocessing\" module_name = \"postprocess_masks\" segmentation_postprocessing_function = getattr ( sys . modules [ __name__ ], method ) parameters_as_string = join_keys_and_values_to_list ( method_parameters ) output_folder_name = join_to_string ( [ module_name , method , * parameters_as_string , data_path . stem ] ) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) directory_ok = check_content_of_two_directories ( data_path , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) segmentation_postprocessing_function ( data_path , output_directory , ** method_parameters ) return output_directory split_tumor_into_core_and_periphery ( input_directory , output_directory , periphery_as_ratio_of_max_distance = 0.2 ) Function which splits tumor into core and periphery based on relative distance from the surface. Parameters input_directory : (pathlib.PosixPath) relative path to the segmented masks. output_directory : (pathlib.PosixPath) relative path where the results should be saved to. periphery_as_ratio_of_max_distance : (float) what ratio of the tumor should be consired core. Any number in <0;1> Returns None: Results are stored on the disk. Example Usage >>> from src.segmentation import split_tumor_into_core_and_periphery >>> split_tumor_into_core_and_periphery ( input_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/segmentation/tumor/segment___thresholding___method-th_triangle' ), output_directory = Path ( 'ppdm/data/my_tumor_folder' ), periphery_as_ratio_of_max_distance = 0.5 ) Source code in src/segmentation_postprocessing.py def split_tumor_into_core_and_periphery ( input_directory : pathlib . PosixPath , output_directory : pathlib . PosixPath , periphery_as_ratio_of_max_distance : float = 0.2 , ) -> None : \"\"\" Function which splits tumor into core and periphery based on relative distance from the surface. Parameters ---------- **input_directory** : *(pathlib.PosixPath)* relative path to the segmented masks. **output_directory** : *(pathlib.PosixPath)* relative path where the results should be saved to. **periphery_as_ratio_of_max_distance**: *(float)* what ratio of the tumor should be consired core. Any number in <0;1> Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import split_tumor_into_core_and_periphery >>>split_tumor_into_core_and_periphery( input_directory = Path('ppdm/data/5IT_DUMMY_STUDY/results/segmentation/tumor/segment___thresholding___method-th_triangle'), output_directory = Path('ppdm/data/my_tumor_folder'), periphery_as_ratio_of_max_distance = 0.5) ``` \"\"\" original_images = sorted ( list ( input_directory . glob ( \"*.npy\" ))) imgs_tumor_mask_list = parallel ( _load_and_transform_image , original_images , n_workers = 12 , threadpool = True , progress = True , ) # check if layers (baesed on image names) are ordered correctly layer_names = [ x [ \"name\" ] for x in imgs_tumor_mask_list ] imgs_tum = _stack_and_downscale_images ( imgs_tumor_mask_list ) # do distance transform imgs_tum_dt = edt . edt ( imgs_tum , black_border = False , parallel = 12 , order = \"C\" ) # split masked object into core and periphery based on distance from surface distance_threshold = periphery_as_ratio_of_max_distance * np . max ( imgs_tum_dt ) imgs_outer = np . where ( imgs_tum_dt < distance_threshold , imgs_tum_dt , 0 ,) # inner mask # make sure inner mask is binary outer_mask = np . where ( imgs_outer > 0 , 1 , 0 ) # outer mask inner_mask = imgs_tum - outer_mask # resize z-axis back to original inner_mask = zoom ( inner_mask , ( 10 , 1 , 1 ), mode = \"nearest\" , order = 0 ) outer_mask = zoom ( outer_mask , ( 10 , 1 , 1 ), mode = \"nearest\" , order = 0 ) # save to disk mask_path = output_directory mask_path . parent . mkdir ( exist_ok = True , parents = True ) inner_mask_iterable = [ x for x in inner_mask ] outer_mask_iterable = [ x for x in outer_mask ] inner_outer_mask_individual_layers = [ { \"name\" : layer_name , \"image_path\" : layer_image , \"core_periphery\" : core_periphery , } for layer_name , layer_image , core_periphery in zip ( layer_names , original_images , zip ( inner_mask_iterable , outer_mask_iterable ), ) ] parallel ( partial ( _encode_combine_transform_and_save_mask , output_directory = mask_path ), inner_outer_mask_individual_layers , n_workers = 12 , progress = True , threadpool = True , )","title":"Segmentation Postprocessing"},{"location":"Modules/segmentation_postprocessing/#segmentation-postprocessing","text":"","title":"Segmentation Postprocessing"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing--segmentation-postprocessing-module","text":"","title":"segmentation postprocessing module:"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing--module-for-certain-masks-postprocessing-operation","text":"","title":"module for certain masks postprocessing operation"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing--implemented-operations-reducing-tumor-borders-and-filling-holes","text":"","title":"Implemented Operations: Reducing Tumor Borders and Filling holes"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing--todo-is-it-discussed-in-the-paper","text":"","title":"TODO: is it discussed in the paper?"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing--this-module-consists-of-two-main-functions","text":"postprocess_masks split_tumor_into_core_and_periphery","title":"This module consists of two main functions:"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing.postprocess_masks","text":"Wrapper function, which handles the masks postprocessing within the pipeline. For now only used for the tumor masks postprocessing to split masks into core and periphery regions.","title":"postprocess_masks()"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing.postprocess_masks--parameters","text":"data_path : (pathlib.PosixPath) relative path to the segmented masks. method : (str) function which should be applied in order to postprocess masks. Thus far we only use 'split_tumor_into_core_and_periphery' function. method_parameters : (Dict) parameters to use for the function applied in method argument.","title":"Parameters"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing.postprocess_masks--returns","text":"output_directory (pathlib.PosixPath) : Path where the results have been stored on the disk to.","title":"Returns"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing.postprocess_masks--example-usage","text":">>> from src.segmentation import postprocess_masks >>> postprocess_masks ( input_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/segmentation/tumor/segment___thresholding___method-th_triangle' ), method = 'split_tumor_into_core_and_periphery' , method_parameters = { 'periphery_as_ratio_of_max_distance' : 0.2 } Source code in src/segmentation_postprocessing.py @log_step def postprocess_masks ( data_path : pathlib . PosixPath , method : str , method_parameters : Dict ) -> pathlib . PosixPath : \"\"\" Wrapper function, which handles the masks postprocessing within the pipeline. For now only used for the tumor masks postprocessing to split masks into core and periphery regions. Parameters ---------- **data_path** : *(pathlib.PosixPath)* relative path to the segmented masks. **method**: *(str)* function which should be applied in order to postprocess masks. Thus far we only use 'split_tumor_into_core_and_periphery' function. **method_parameters**: *(Dict)* parameters to use for the function applied in **method** argument. Returns ------ output_directory *(pathlib.PosixPath)*: Path where the results have been stored on the disk to. Example Usage -------------- ```python >>>from src.segmentation import postprocess_masks >>>postprocess_masks( input_directory = Path('ppdm/data/5IT_DUMMY_STUDY/results/segmentation/tumor/segment___thresholding___method-th_triangle'), method = 'split_tumor_into_core_and_periphery', method_parameters = {'periphery_as_ratio_of_max_distance': 0.2} ``` \"\"\" module_results_path = \"results/segmentation_postprocessing\" module_name = \"postprocess_masks\" segmentation_postprocessing_function = getattr ( sys . modules [ __name__ ], method ) parameters_as_string = join_keys_and_values_to_list ( method_parameters ) output_folder_name = join_to_string ( [ module_name , method , * parameters_as_string , data_path . stem ] ) output_directory = create_relative_path ( data_path , module_results_path , output_folder_name , _infer_root_based_on = \"results\" , ) directory_ok = check_content_of_two_directories ( data_path , output_directory ) if directory_ok is False : if output_directory . exists (): remove_content ( output_directory ) segmentation_postprocessing_function ( data_path , output_directory , ** method_parameters ) return output_directory","title":"Example Usage"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing.split_tumor_into_core_and_periphery","text":"Function which splits tumor into core and periphery based on relative distance from the surface.","title":"split_tumor_into_core_and_periphery()"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing.split_tumor_into_core_and_periphery--parameters","text":"input_directory : (pathlib.PosixPath) relative path to the segmented masks. output_directory : (pathlib.PosixPath) relative path where the results should be saved to. periphery_as_ratio_of_max_distance : (float) what ratio of the tumor should be consired core. Any number in <0;1>","title":"Parameters"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing.split_tumor_into_core_and_periphery--returns","text":"None: Results are stored on the disk.","title":"Returns"},{"location":"Modules/segmentation_postprocessing/#src.segmentation_postprocessing.split_tumor_into_core_and_periphery--example-usage","text":">>> from src.segmentation import split_tumor_into_core_and_periphery >>> split_tumor_into_core_and_periphery ( input_directory = Path ( 'ppdm/data/5IT_DUMMY_STUDY/results/segmentation/tumor/segment___thresholding___method-th_triangle' ), output_directory = Path ( 'ppdm/data/my_tumor_folder' ), periphery_as_ratio_of_max_distance = 0.5 ) Source code in src/segmentation_postprocessing.py def split_tumor_into_core_and_periphery ( input_directory : pathlib . PosixPath , output_directory : pathlib . PosixPath , periphery_as_ratio_of_max_distance : float = 0.2 , ) -> None : \"\"\" Function which splits tumor into core and periphery based on relative distance from the surface. Parameters ---------- **input_directory** : *(pathlib.PosixPath)* relative path to the segmented masks. **output_directory** : *(pathlib.PosixPath)* relative path where the results should be saved to. **periphery_as_ratio_of_max_distance**: *(float)* what ratio of the tumor should be consired core. Any number in <0;1> Returns ------ None: Results are stored on the disk. Example Usage -------------- ```python >>>from src.segmentation import split_tumor_into_core_and_periphery >>>split_tumor_into_core_and_periphery( input_directory = Path('ppdm/data/5IT_DUMMY_STUDY/results/segmentation/tumor/segment___thresholding___method-th_triangle'), output_directory = Path('ppdm/data/my_tumor_folder'), periphery_as_ratio_of_max_distance = 0.5) ``` \"\"\" original_images = sorted ( list ( input_directory . glob ( \"*.npy\" ))) imgs_tumor_mask_list = parallel ( _load_and_transform_image , original_images , n_workers = 12 , threadpool = True , progress = True , ) # check if layers (baesed on image names) are ordered correctly layer_names = [ x [ \"name\" ] for x in imgs_tumor_mask_list ] imgs_tum = _stack_and_downscale_images ( imgs_tumor_mask_list ) # do distance transform imgs_tum_dt = edt . edt ( imgs_tum , black_border = False , parallel = 12 , order = \"C\" ) # split masked object into core and periphery based on distance from surface distance_threshold = periphery_as_ratio_of_max_distance * np . max ( imgs_tum_dt ) imgs_outer = np . where ( imgs_tum_dt < distance_threshold , imgs_tum_dt , 0 ,) # inner mask # make sure inner mask is binary outer_mask = np . where ( imgs_outer > 0 , 1 , 0 ) # outer mask inner_mask = imgs_tum - outer_mask # resize z-axis back to original inner_mask = zoom ( inner_mask , ( 10 , 1 , 1 ), mode = \"nearest\" , order = 0 ) outer_mask = zoom ( outer_mask , ( 10 , 1 , 1 ), mode = \"nearest\" , order = 0 ) # save to disk mask_path = output_directory mask_path . parent . mkdir ( exist_ok = True , parents = True ) inner_mask_iterable = [ x for x in inner_mask ] outer_mask_iterable = [ x for x in outer_mask ] inner_outer_mask_individual_layers = [ { \"name\" : layer_name , \"image_path\" : layer_image , \"core_periphery\" : core_periphery , } for layer_name , layer_image , core_periphery in zip ( layer_names , original_images , zip ( inner_mask_iterable , outer_mask_iterable ), ) ] parallel ( partial ( _encode_combine_transform_and_save_mask , output_directory = mask_path ), inner_outer_mask_individual_layers , n_workers = 12 , progress = True , threadpool = True , )","title":"Example Usage"},{"location":"Overview/intro/","text":"This is a intor file for i in range ( 20 ): print ( i )","title":"This is a intor file"},{"location":"Overview/intro/#this-is-a-intor-file","text":"for i in range ( 20 ): print ( i )","title":"This is a intor file"}]}